{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('/Users/markjos/projects/malachor5')\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from glob import glob\n",
    "import math\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import sys\n",
    "sys.path.append('scripts')\n",
    "from eval import get_word_language\n",
    "from longform import load_and_resample\n",
    "import torchaudio\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(109393, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_path</th>\n",
       "      <th>tier_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>eaf_path</th>\n",
       "      <th>wav_source</th>\n",
       "      <th>sli_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>30</td>\n",
       "      <td>1414</td>\n",
       "      <td>Hello, hello, hello.</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2106</td>\n",
       "      <td>4755</td>\n",
       "      <td>Hello, hello, hello, hello, hello.</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>6240</td>\n",
       "      <td>8502</td>\n",
       "      <td>Hello, hello. This shit.</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>8535</td>\n",
       "      <td>8569</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>8721</td>\n",
       "      <td>11269</td>\n",
       "      <td>Hello. Hello. Hello.</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  wav_path tier_name  start    end                        transcription  \\\n",
       "0                asr     30   1414                 Hello, hello, hello.   \n",
       "1                asr   2106   4755   Hello, hello, hello, hello, hello.   \n",
       "2                asr   6240   8502             Hello, hello. This shit.   \n",
       "3                asr   8535   8569                                  ...   \n",
       "4                asr   8721  11269                 Hello. Hello. Hello.   \n",
       "\n",
       "                                            eaf_path  \\\n",
       "0  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "1  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "2  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "3  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "4  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "\n",
       "                                          wav_source sli_pred  \n",
       "0  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG  \n",
       "1  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG  \n",
       "2  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG  \n",
       "3  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG  \n",
       "4  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longform_csv_path = 'data/elicitation-wavs/autotranscribed/metadata.csv'\n",
    "df = pd.read_csv(longform_csv_path, index_col=0, keep_default_na=False)\n",
    "print(f\"{df.shape=}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper output cleaning\n",
    "Figure out what labels are excessive repetitions and exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              hello hello hello\n",
       "1                  hello hello hello hello hello\n",
       "2                          hello hello this shit\n",
       "3                                              .\n",
       "4                              hello hello hello\n",
       "                         ...                    \n",
       "767                                         okay\n",
       "768                                         yeah\n",
       "769    but this really studied with the question\n",
       "770                                         okay\n",
       "771             yes i am going to stop recording\n",
       "Name: normalized, Length: 109393, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-medium')\n",
    "df['normalized']=df['transcription'].apply(tokenizer.normalize)\n",
    "df['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['basic_normalized']=df['transcription'].apply(tokenizer.basic_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transcription']=df['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng_only\n",
       "True     80307\n",
       "False    29086\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_only = lambda s: all(get_word_language(word) == 'eng' for word in s.split())\n",
    "df['eng_only'] = df['transcription'].apply(eng_only)\n",
    "df['eng_only'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of words per sentence\n",
    "# and number of times most common word repeats\n",
    "df['num_words']=df['transcription'].str.split().apply(len)\n",
    "def get_most_freq_word_count(s:str):\n",
    "    words = s.split()\n",
    "    words = [x.lower() for x in words]\n",
    "    if len(words)==0:\n",
    "        return 0\n",
    "    return max([words.count(w) for w in words])\n",
    "df['max_word_count'] = df['transcription'].apply(get_most_freq_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_word_count\n",
       "True     105200\n",
       "False      4193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['max_word_count']<5).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_punct\n",
       "False    104488\n",
       "True       4905\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_is_punct = lambda s: all(c in punctuation for c in s.strip())\n",
    "df['is_punct'] = df['transcription'].apply(str_is_punct)\n",
    "df['is_punct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    75955\n",
       "True     33438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sli_eng_mask = df['sli_pred']=='ENG'\n",
    "discard_mask = (~sli_eng_mask | (~df['eng_only']) | (df['max_word_count']>=5) | (df['is_punct']))\n",
    "discard_mask.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe with metadata for English annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "snippet_metadata_path = '/Users/markjos/projects/malachor5/notebooks/longform_dataset/snippets_metadata.csv'\n",
    "df.to_csv(snippet_metadata_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47326, 14)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid=df[~discard_mask]\n",
    "df_valid = df_valid.drop_duplicates('transcription')\n",
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      21\n",
       "1      35\n",
       "2      25\n",
       "4      21\n",
       "6       6\n",
       "       ..\n",
       "765    19\n",
       "767     6\n",
       "768     6\n",
       "769    43\n",
       "770     6\n",
       "Name: charlen, Length: 74315, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charlen'] = df['transcription'].str.len()\n",
    "df['charlen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    74315.000000\n",
       "mean         3.185645\n",
       "std          0.710206\n",
       "min          0.000000\n",
       "25%          2.584963\n",
       "50%          3.337175\n",
       "75%          3.733661\n",
       "max          4.430225\n",
       "Name: entropy, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(string):\n",
    "    \"\"\"Calculates the Shannon entropy of a string.\"\"\"\n",
    "\n",
    "    # Get the probability of each character in the string\n",
    "    prob = {}\n",
    "    for c in string:\n",
    "        if c not in prob:\n",
    "            prob[c] = 0\n",
    "        prob[c] += 1\n",
    "    for c in prob:\n",
    "        prob[c] /= len(string)\n",
    "\n",
    "    # Calculate the entropy\n",
    "    entropy = -sum(p * math.log2(p) for p in prob.values())\n",
    "\n",
    "    return entropy\n",
    "df['entropy']=df['transcription'].apply(entropy)\n",
    "df['entropy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAROElEQVR4nO3df6zddX3H8edrVERloyDmhrTNSmLjghAd3kAXkuUKGxQwlj/UYJgU061/DB0uJFqWLM1UEkycCMs0aaQTHLEyNKGxbqwBbsySgVBhIFRCg0XaIFVbqsVfq3vvj/NpPfTeQu85t/ecnvt8JCf3+/18f5z3/bT3vO7n8/2ec1NVSJLmt98bdAGSpMEzDCRJhoEkyTCQJGEYSJKABYMuoFenn356LV26tKdjX375Zd70pjfNbkHHMftjKvtkKvtkquOtT7Zu3fqTqnrLdNuO2zBYunQpjzzySE/HTk5OMjExMbsFHcfsj6nsk6nsk6mOtz5J8tyRtjlNJEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkjuN3IEvz1dK1mw8t77jp8gFWolHiyECSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjiKMEiyIcnuJN/rajstyZYkz7Svp7b2JLk1yfYkjyc5t+uYVW3/Z5Ks6mp/V5In2jG3Jslsf5OSpFd3NCODLwMrDmtbC9xXVcuA+9o6wKXAsvZYA3wROuEBrAPOB84D1h0MkLbPX3Udd/hzSZKOsdcMg6r6NrDnsOaVwO1t+Xbgiq72O6rjQWBhkjOAS4AtVbWnqvYCW4AVbdsfVNWDVVXAHV3nkiTNkV7/BvJYVb3Qln8EjLXlRcDzXfvtbG2v1r5zmvZpJVlDZ8TB2NgYk5OTPRW/f//+no8dRfbHVMPcJ9efc+DQ8lzWOMx9Miij1Ce9hsEhVVVJajaKOYrnWg+sBxgfH6+JiYmezjM5OUmvx44i+2OqYe6Ta9ZuPrS846qJOXveYe6TQRmlPun1bqIX2xQP7evu1r4LWNK13+LW9mrti6dplyTNoV7DYBNw8I6gVcA9Xe1Xt7uKlgP72nTSvcDFSU5tF44vBu5t236WZHm7i+jqrnNJkubIa04TJfkqMAGcnmQnnbuCbgLuSrIaeA74QNv9W8BlwHbgF8CHAapqT5JPAQ+3/T5ZVQcvSv81nTuW3gD8e3tIkubQa4ZBVX3wCJsummbfAq49wnk2ABumaX8EOPu16pAkHTu+A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJPsMgyd8meTLJ95J8NclJSc5M8lCS7Um+luTEtu/r2/r2tn1p13luaO1PJ7mkz+9JkjRDPYdBkkXA3wDjVXU2cAJwJfAZ4OaqeiuwF1jdDlkN7G3tN7f9SHJWO+7twArgC0lO6LUuSdLM9TtNtAB4Q5IFwBuBF4ALgbvb9tuBK9ryyrZO235RkrT2jVX166r6AbAdOK/PuiRJM7Cg1wOraleSzwI/BH4J/CewFXipqg603XYCi9ryIuD5duyBJPuAN7f2B7tO3X3MKyRZA6wBGBsbY3Jysqfa9+/f3/Oxo8j+mGqY++T6cw4cWp7LGoe5TwZllPqk5zBIciqd3+rPBF4C/o3ONM8xU1XrgfUA4+PjNTEx0dN5Jicn6fXYUWR/TDXMfXLN2s2HlndcNTFnzzvMfTIoo9Qn/UwT/Rnwg6r6cVX9L/AN4AJgYZs2AlgM7GrLu4AlAG37KcBPu9unOUaSNAf6CYMfAsuTvLHN/V8EPAU8ALyv7bMKuKctb2rrtO33V1W19ivb3UZnAsuA7/RRlyRphvq5ZvBQkruB7wIHgEfpTOFsBjYm+XRru60dchvwlSTbgT107iCiqp5MchedIDkAXFtVv+21LknSzPUcBgBVtQ5Yd1jzs0xzN1BV/Qp4/xHOcyNwYz+1SJJ65zuQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0GQZJFia5O8n3k2xL8idJTkuyJckz7eupbd8kuTXJ9iSPJzm36zyr2v7PJFnV7zclSZqZfkcGtwD/UVV/BLwD2AasBe6rqmXAfW0d4FJgWXusAb4IkOQ0YB1wPnAesO5ggEiS5kbPYZDkFOBPgdsAquo3VfUSsBK4ve12O3BFW14J3FEdDwILk5wBXAJsqao9VbUX2AKs6LUuSdLMLejj2DOBHwP/kuQdwFbgOmCsql5o+/wIGGvLi4Dnu47f2dqO1D5FkjV0RhWMjY0xOTnZU+H79+/v+dhRZH9MNcx9cv05Bw4tz2WNw9wngzJKfdJPGCwAzgU+WlUPJbmF300JAVBVlaT6KfCw860H1gOMj4/XxMRET+eZnJyk12NHkf0x1TD3yTVrNx9a3nHVxJw97zD3yaCMUp/0c81gJ7Czqh5q63fTCYcX2/QP7evutn0XsKTr+MWt7UjtkqQ50nMYVNWPgOeTvK01XQQ8BWwCDt4RtAq4py1vAq5udxUtB/a16aR7gYuTnNouHF/c2iRJc6SfaSKAjwJ3JjkReBb4MJ2AuSvJauA54ANt328BlwHbgV+0famqPUk+BTzc9vtkVe3psy5J0gz0FQZV9RgwPs2mi6bZt4Brj3CeDcCGfmqRJPXOdyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkMQthkOSEJI8m+WZbPzPJQ0m2J/lakhNb++vb+va2fWnXOW5o7U8nuaTfmiRJMzMbI4PrgG1d658Bbq6qtwJ7gdWtfTWwt7Xf3PYjyVnAlcDbgRXAF5KcMAt1SZKOUl9hkGQxcDnwpbYe4ELg7rbL7cAVbXllW6dtv6jtvxLYWFW/rqofANuB8/qpS5I0Mwv6PP7zwMeB32/rbwZeqqoDbX0nsKgtLwKeB6iqA0n2tf0XAQ92nbP7mFdIsgZYAzA2Nsbk5GRPRe/fv7/nY0eR/THVMPfJ9eccOLQ8lzUOc58Myij1Sc9hkOQ9wO6q2ppkYtYqehVVtR5YDzA+Pl4TE7097eTkJL0eO4rsj6mGuU+uWbv50PKOqybm7HmHuU8GZZT6pJ+RwQXAe5NcBpwE/AFwC7AwyYI2OlgM7Gr77wKWADuTLABOAX7a1X5Q9zGSpDnQ8zWDqrqhqhZX1VI6F4Dvr6qrgAeA97XdVgH3tOVNbZ22/f6qqtZ+Zbvb6ExgGfCdXuuSJM1cv9cMpvMJYGOSTwOPAre19tuAryTZDuyhEyBU1ZNJ7gKeAg4A11bVb49BXZKkI5iVMKiqSWCyLT/LNHcDVdWvgPcf4fgbgRtnoxZJ0sz5DmRJkmEgSTIMJEkcmwvIkqaxtPv9ATddPsBKpKkcGUiSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk/HsGkuaR7r8pAf5diW6ODCRJhoEkyWkiSfPY4dNGB83H6SPDQNJIO9ILvl7JMJCkw3QHyHwZJRgGkubEsX6BdQTQH8NA0nHFF/1jwzCQpvHErn1c01505ss0gaY3X6aMer61NMmSJA8keSrJk0mua+2nJdmS5Jn29dTWniS3Jtme5PEk53ada1Xb/5kkq/r/tiRJM9HP+wwOANdX1VnAcuDaJGcBa4H7qmoZcF9bB7gUWNYea4AvQic8gHXA+cB5wLqDASJJmhs9h0FVvVBV323LPwe2AYuAlcDtbbfbgSva8krgjup4EFiY5AzgEmBLVe2pqr3AFmBFr3VJkmYuVdX/SZKlwLeBs4EfVtXC1h5gb1UtTPJN4Kaq+q+27T7gE8AEcFJVfbq1/z3wy6r67DTPs4bOqIKxsbF3bdy4sad69+/fz8knn9zTsaPI/phq9559vPjLzvI5i06ZlXM+sWvfoeV+zjlb55mpfv+fHIvvf64dXvfx9rPz7ne/e2tVjU+3re8LyElOBr4OfKyqftZ5/e+oqkrSf9r87nzrgfUA4+PjNTEx0dN5Jicn6fXYUWR/TPVPd97DPz7R+fHYcdXErJzzmu4LkX2cc7bOM1P9/j/pp+5X3kE0wPtennj50OKOmy4fqZ+dvj6bKMnr6ATBnVX1jdb8Ypv+oX3d3dp3AUu6Dl/c2o7ULkmaI/3cTRTgNmBbVX2ua9Mm4OAdQauAe7rar253FS0H9lXVC8C9wMVJTm0Xji9ubZLmgaVrNx96aHD6GW9dAHwIeCLJY63t74CbgLuSrAaeAz7Qtn0LuAzYDvwC+DBAVe1J8ing4bbfJ6tqTx91STpOzZd7+odRz2HQLgTnCJsvmmb/Aq49wrk2ABt6rUWS1B//noEkyY+jkDSchv0awtK1m7n+nANcs3bzSExpOTKQJBkGkiSniST1yDt/RosjA0mSIwNJs8sRw/HJkYEkyZGBpP4N+22gx9oojIYMA0nAaLygqXdOE0mS5mcYPLFrn5+SKEld5mUYSJJeyWsG0pByDl9zyTCQ9KoOhtLBD2WbzXOOouM1xJ0mkiQ5MpDmm+P1N1cdW44MJEmODCRNNVtz+qN8bWDUGAbSiPMFWUfDMJBGkAEwHI6n6zOGgTTLjqcXAENDB3kBWZJkGEiSDANJEl4zkKQ5MezXkhwZSJIcGWh49fub1LD/JjYbvBtIs8WRgSRpeEYGSVYAtwAnAF+qqpsGXJLmgX5GD/Nh5KFjYxj/7wxFGCQ5Afhn4M+BncDDSTZV1VODrUw69obxhUFzZ1j+/YciDIDzgO1V9SxAko3ASsAw0HFtpnP6R9rfawPzwyCDIVU1p084bRHJ+4AVVfWXbf1DwPlV9ZHD9lsDrGmrbwOe7vEpTwd+0uOxo8j+mMo+mco+mep465M/rKq3TLdhWEYGR6Wq1gPr+z1PkkeqanwWShoJ9sdU9slU9slUo9Qnw3I30S5gSdf64tYmSZoDwxIGDwPLkpyZ5ETgSmDTgGuSpHljKKaJqupAko8A99K5tXRDVT15DJ+y76mmEWN/TGWfTGWfTDUyfTIUF5AlSYM1LNNEkqQBMgwkSfMrDJKsSPJ0ku1J1g66nkFLsiHJ7iTfG3QtwyLJkiQPJHkqyZNJrht0TYOW5KQk30nyP61P/mHQNQ2DJCckeTTJNwddy2yYN2HQ9ZEXlwJnAR9MctZgqxq4LwMrBl3EkDkAXF9VZwHLgWv9f8KvgQur6h3AO4EVSZYPtqShcB2wbdBFzJZ5EwZ0feRFVf0GOPiRF/NWVX0b2DPoOoZJVb1QVd9tyz+n88O+aLBVDVZ17G+rr2uPeX3nSZLFwOXAlwZdy2yZT2GwCHi+a30n8/yHXK8uyVLgj4GHBlzKwLUpkceA3cCWqprvffJ54OPA/w24jlkzn8JAOmpJTga+Dnysqn426HoGrap+W1XvpPPpAOclOXvAJQ1MkvcAu6tq66BrmU3zKQz8yAsdlSSvoxMEd1bVNwZdzzCpqpeAB5jf15ouAN6bZAed6eYLk/zrYEvq33wKAz/yQq8pSYDbgG1V9blB1zMMkrwlycK2/AY6f3fk+wMtaoCq6oaqWlxVS+m8jtxfVX8x4LL6Nm/CoKoOAAc/8mIbcNcx/siLoZfkq8B/A29LsjPJ6kHXNAQuAD5E57e9x9rjskEXNWBnAA8keZzOL1VbqmokbqfU7/hxFJKk+TMykCQdmWEgSTIMJEmGgSQJw0CShGEgScIwkCQB/w8/uHVkdUMKEQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['entropy'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['count_most_frequent_word'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>charlen</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788475</td>\n",
       "      <td>0.518018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charlen</th>\n",
       "      <td>0.788475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.518018</td>\n",
       "      <td>0.692197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          duration   charlen   entropy\n",
       "duration  1.000000  0.788475  0.518018\n",
       "charlen   0.788475  1.000000  0.692197\n",
       "entropy   0.518018  0.692197  1.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['duration']=df['end']-df['start']\n",
    "df[['duration', 'charlen', 'entropy']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Uh...', '', ' So...', ' Hmm.', ' See?', ' do do do.', ' m.',\n",
       "       ' Um...', ' I.', ' We...', ' if', ' If...', ' goo goo.', ' uh...',\n",
       "       ' SSA.', ' add', ' Hmm?', ' E.M.', ' Is...', ' L.A.', ' X.',\n",
       "       ' See.', ' H.', ' G.', ' Z.', ' Is', ' A.', ' Q.', ' uh',\n",
       "       ' la la.', ' Um', ' R. R. A.', ' J.', ' M O.', ' E.R.E.', ' H. R.',\n",
       "       ' do', ' p', ' E.', ' R.E.', ' G. R.', ' W.', ' L.', ' La La.',\n",
       "       ' d.', ' E. R.', ' egg.', ' L. A.', ' In', ' Z. A.', ' A B.',\n",
       "       ' Oh', ' G.I.', ' or', ' in', ' shh.', 'lòlò lòlò', ' a',\n",
       "       ' do do do do.', ' is', ' see.', ' I...', ' La La La.', ' to',\n",
       "       ' d', ' R.', ' See I', ' ...so...', ' v.', ' V.', ' Boo.',\n",
       "       ' is...', ' To...', ' N.', ' W. A.', ' ...Otto...', ' Or...',\n",
       "       ' will', ' la la la.', ' ...for...', ' or...', ' GD...', ' V.P.',\n",
       "       ' Nee.', ' h', ' ...age...', ' eye.', ' Y.', ' so', ' Hmm...',\n",
       "       ' I. S.', ' re... re... re...', ' odd.', ' cool', ' F.', ' I. D.',\n",
       "       ' s', ' um', ' of', ' D.', ' G.O.', ' I', ' T.', ' L T.', ' M.',\n",
       "       ' me', ' Wee.', ' all', ' E D.', ' L A.', ' H. I.', ' ABA.',\n",
       "       ' A R.'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['entropy']<2, 'transcription'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    73219.000000\n",
       "mean        15.747412\n",
       "std          9.860388\n",
       "min          0.984865\n",
       "25%          7.390085\n",
       "50%         12.495746\n",
       "75%         25.647680\n",
       "max         36.133876\n",
       "Name: entropy_over_charlen, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charlen_nmzd'] = df['charlen'] / df['charlen'].max()\n",
    "df['entropy_nmzd'] = df['entropy'] / df['entropy'].max()\n",
    "df['entropy_over_charlen'] = df['entropy_nmzd'] / df['charlen_nmzd']\n",
    "df['entropy_over_charlen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('entropy_over_charlen', ascending=True).iloc[-5:-1]['transcription'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" You could even type these, or I'll fix them afterwards, but I'll... copy link.\",\n",
       " \" Yeah, and Merrick does this as well. It's so confusing for English speakers.\",\n",
       " \" Okay, so let's break it down. Yeah, exactly. Yeah, I came to speak with you.\",\n",
       " ' All right. I mean, maybe, maybe you could just keep screen sharing. So yeah.',\n",
       " \" We're using this funny looking and same for like the this open. Oh, whoops.\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff_idx = 3_566\n",
    "df_sorted=df.sort_values('entropy_over_charlen', ascending=True)\n",
    "df_cutoff=df_sorted.iloc[cutoff_idx:]\n",
    "df_cutoff.sort_values('charlen', ascending=False).head()['transcription'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>charlen</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.687693</td>\n",
       "      <td>0.496942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charlen</th>\n",
       "      <td>0.687693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.496942</td>\n",
       "      <td>0.773562</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          duration   charlen   entropy\n",
       "duration  1.000000  0.687693  0.496942\n",
       "charlen   0.687693  1.000000  0.773562\n",
       "entropy   0.496942  0.773562  1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cutoff[['duration', 'charlen', 'entropy']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_punct = df_cutoff['transcription'].str.strip().apply(str_is_punct)\n",
    "df_cutoff[is_punct].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75955, 109393, 0.6943314471675519)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_valid = df_cutoff[~is_punct].copy()\n",
    "df_valid.shape[0], df.shape[0], df_valid.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('entropy_over_charlen').to_csv('data/elicitation-wavs/autotranscribed/metadata_sorted_by_entropy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_path</th>\n",
       "      <th>tier_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>eaf_path</th>\n",
       "      <th>wav_source</th>\n",
       "      <th>sli_pred</th>\n",
       "      <th>eng_only</th>\n",
       "      <th>num_words</th>\n",
       "      <th>max_word_count</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>normalized</th>\n",
       "      <th>basic_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>902522</td>\n",
       "      <td>907804</td>\n",
       "      <td>apri vəlɛdɛ vəlɛdɛ javəlɛdɛ</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>apri vəlɛdɛ vəlɛdɛ javəlɛdɛ</td>\n",
       "      <td>apri vəlɛdɛ vəlɛdɛ javəlɛdɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>1649663</td>\n",
       "      <td>1652329</td>\n",
       "      <td>da ɔɟɔ vəlɛdɛ unɛɾɛ</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>da ɔɟɔ vəlɛdɛ unɛɾɛ</td>\n",
       "      <td>da ɔɟɔ vəlɛdɛ unɛɾɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>1807782</td>\n",
       "      <td>1809250</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>1858373</td>\n",
       "      <td>1859352</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "      <td>ndɔbagɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>1920844</td>\n",
       "      <td>1922802</td>\n",
       "      <td>an ɔɟɔ kavəlɛdɔ ndɔbagɛ</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>an ɔɟɔ kavəlɛdɔ ndɔbagɛ</td>\n",
       "      <td>an ɔɟɔ kavəlɛdɔ ndɔbagɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2278257</td>\n",
       "      <td>2280822</td>\n",
       "      <td>an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare</td>\n",
       "      <td>an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2331059</td>\n",
       "      <td>2332358</td>\n",
       "      <td>lagɛda ndɔdɔ tɛlubu</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>lagɛda ndɔdɔ tɛlubu</td>\n",
       "      <td>lagɛda ndɔdɔ tɛlubu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2469434</td>\n",
       "      <td>2470682</td>\n",
       "      <td>dɛbulti</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>dɛbulti</td>\n",
       "      <td>dɛbulti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2498762</td>\n",
       "      <td>2500804</td>\n",
       "      <td>dəvəɽa dɛr ive</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>dəvəɽa dɛr ive</td>\n",
       "      <td>dəvəɽa dɛr ive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td></td>\n",
       "      <td>asr</td>\n",
       "      <td>2804014</td>\n",
       "      <td>2806292</td>\n",
       "      <td>kagɛ dɔndɔ idɛtɛ lʊbu</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>TIC</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>kagɛ dɔndɔ idɛtɛ lʊbu</td>\n",
       "      <td>kagɛ dɔndɔ idɛtɛ lʊbu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1343 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    wav_path tier_name    start      end  \\\n",
       "213                asr   902522   907804   \n",
       "402                asr  1649663  1652329   \n",
       "451                asr  1807782  1809250   \n",
       "461                asr  1858373  1859352   \n",
       "476                asr  1920844  1922802   \n",
       "..       ...       ...      ...      ...   \n",
       "515                asr  2278257  2280822   \n",
       "528                asr  2331059  2332358   \n",
       "567                asr  2469434  2470682   \n",
       "575                asr  2498762  2500804   \n",
       "664                asr  2804014  2806292   \n",
       "\n",
       "                                   transcription  \\\n",
       "213                  apri vəlɛdɛ vəlɛdɛ javəlɛdɛ   \n",
       "402                          da ɔɟɔ vəlɛdɛ unɛɾɛ   \n",
       "451                                      ndɔbagɛ   \n",
       "461                                      ndɔbagɛ   \n",
       "476                      an ɔɟɔ kavəlɛdɔ ndɔbagɛ   \n",
       "..                                           ...   \n",
       "515  an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare   \n",
       "528                          lagɛda ndɔdɔ tɛlubu   \n",
       "567                                      dɛbulti   \n",
       "575                               dəvəɽa dɛr ive   \n",
       "664                        kagɛ dɔndɔ idɛtɛ lʊbu   \n",
       "\n",
       "                                              eaf_path  \\\n",
       "213  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "402  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "451  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "461  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "476  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "..                                                 ...   \n",
       "515  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "528  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "567  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "575  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "664  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "\n",
       "                                            wav_source sli_pred  eng_only  \\\n",
       "213  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "402  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "451  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "461  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "476  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "..                                                 ...      ...       ...   \n",
       "515  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "528  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "567  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "575  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "664  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      TIC     False   \n",
       "\n",
       "     num_words  max_word_count  is_punct  \\\n",
       "213          4               2     False   \n",
       "402          4               1     False   \n",
       "451          1               1     False   \n",
       "461          1               1     False   \n",
       "476          4               1     False   \n",
       "..         ...             ...       ...   \n",
       "515          8               1     False   \n",
       "528          3               1     False   \n",
       "567          1               1     False   \n",
       "575          3               1     False   \n",
       "664          4               1     False   \n",
       "\n",
       "                                      normalized  \\\n",
       "213                  apri vəlɛdɛ vəlɛdɛ javəlɛdɛ   \n",
       "402                          da ɔɟɔ vəlɛdɛ unɛɾɛ   \n",
       "451                                      ndɔbagɛ   \n",
       "461                                      ndɔbagɛ   \n",
       "476                      an ɔɟɔ kavəlɛdɔ ndɔbagɛ   \n",
       "..                                           ...   \n",
       "515  an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare   \n",
       "528                          lagɛda ndɔdɔ tɛlubu   \n",
       "567                                      dɛbulti   \n",
       "575                               dəvəɽa dɛr ive   \n",
       "664                        kagɛ dɔndɔ idɛtɛ lʊbu   \n",
       "\n",
       "                                basic_normalized  \n",
       "213                  apri vəlɛdɛ vəlɛdɛ javəlɛdɛ  \n",
       "402                          da ɔɟɔ vəlɛdɛ unɛɾɛ  \n",
       "451                                      ndɔbagɛ  \n",
       "461                                      ndɔbagɛ  \n",
       "476                      an ɔɟɔ kavəlɛdɔ ndɔbagɛ  \n",
       "..                                           ...  \n",
       "515  an ndɔn kaɾogɛ apri dɔndɔdɔ idɔtɔ ləvɔ kare  \n",
       "528                          lagɛda ndɔdɔ tɛlubu  \n",
       "567                                      dɛbulti  \n",
       "575                               dəvəɽa dɛr ive  \n",
       "664                        kagɛ dɔndɔ idɛtɛ lʊbu  \n",
       "\n",
       "[1343 rows x 14 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sli_pred']=='TIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63710, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tier_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>eaf_path</th>\n",
       "      <th>wav_source</th>\n",
       "      <th>sli_pred</th>\n",
       "      <th>asr_index</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asr</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1414.0</td>\n",
       "      <td>hello hello hello</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asr</td>\n",
       "      <td>6240.0</td>\n",
       "      <td>8502.0</td>\n",
       "      <td>hello hello this shit</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asr</td>\n",
       "      <td>11387.0</td>\n",
       "      <td>13244.0</td>\n",
       "      <td>i do not know</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asr</td>\n",
       "      <td>13649.0</td>\n",
       "      <td>13986.0</td>\n",
       "      <td>yeah</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asr</td>\n",
       "      <td>26625.0</td>\n",
       "      <td>27064.0</td>\n",
       "      <td>thank you</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tier_name    start      end          transcription  \\\n",
       "0       asr     30.0   1414.0      hello hello hello   \n",
       "1       asr   6240.0   8502.0  hello hello this shit   \n",
       "2       asr  11387.0  13244.0          i do not know   \n",
       "3       asr  13649.0  13986.0                   yeah   \n",
       "4       asr  26625.0  27064.0              thank you   \n",
       "\n",
       "                                            eaf_path  \\\n",
       "0  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "1  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "2  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "3  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "4  /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "\n",
       "                                          wav_source sli_pred  asr_index split  \n",
       "0  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   NaN  \n",
       "1  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   NaN  \n",
       "2  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   NaN  \n",
       "3  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   NaN  \n",
       "4  /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   NaN  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_csv_path = 'data/hf-datasets/tira-clean/metadata.csv'\n",
    "asr_df = pd.read_csv(asr_csv_path, index_col=0, keep_default_na=False)\n",
    "asr_df=asr_df.reset_index(names='asr_index')\n",
    "asr_df['tier_name']='human_label'\n",
    "asr_df['eaf_path']=asr_df['eaf_source']\n",
    "asr_df['sli_pred']=np.nan\n",
    "asr_df=asr_df[asr_df['split']=='train']\n",
    "\n",
    "df_valid['asr_index']=np.nan\n",
    "df_valid['split']=np.nan\n",
    "\n",
    "merge_cols = [\n",
    "    'tier_name', 'start', 'end',\n",
    "    'transcription', 'eaf_path',\n",
    "    'wav_source', 'sli_pred',\n",
    "    'asr_index', 'split',\n",
    "]\n",
    "\n",
    "merged_df = pd.concat([df_valid[merge_cols], asr_df[merge_cols]], ignore_index=True)\n",
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tier_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>eaf_path</th>\n",
       "      <th>wav_source</th>\n",
       "      <th>sli_pred</th>\n",
       "      <th>asr_index</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75955</th>\n",
       "      <td>human_label</td>\n",
       "      <td>2497647.0</td>\n",
       "      <td>2498747.0</td>\n",
       "      <td>lə̀və̀lɛ̀ðɛ́l únɛ̀ɾɛ̀</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75956</th>\n",
       "      <td>human_label</td>\n",
       "      <td>2325216.0</td>\n",
       "      <td>2326306.0</td>\n",
       "      <td>kə̀ŋàcîí</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75957</th>\n",
       "      <td>human_label</td>\n",
       "      <td>354974.0</td>\n",
       "      <td>356382.0</td>\n",
       "      <td>ŋ̀gátɛ́və́lɛ̂ðɔ́ nd̪ɔ̀bàgɛ̀</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75958</th>\n",
       "      <td>human_label</td>\n",
       "      <td>154905.0</td>\n",
       "      <td>157215.0</td>\n",
       "      <td>ŋɔ́ðɔ́ ŋá və́lɛ̀ðà ðàŋàlà nd̪ɔ̀bà</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75959</th>\n",
       "      <td>human_label</td>\n",
       "      <td>2311065.0</td>\n",
       "      <td>2312385.0</td>\n",
       "      <td>ðə̀və̀lèðɔ̀ŋ</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96430</th>\n",
       "      <td>human_label</td>\n",
       "      <td>1005320.0</td>\n",
       "      <td>1007400.0</td>\n",
       "      <td>ðá nɛ́lê və̀lɛ̀ðɔ́ nd̪ɔ̀bà</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20475.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96431</th>\n",
       "      <td>human_label</td>\n",
       "      <td>997005.0</td>\n",
       "      <td>997694.0</td>\n",
       "      <td>ŋə̀vrà</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20476.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96432</th>\n",
       "      <td>human_label</td>\n",
       "      <td>1121405.0</td>\n",
       "      <td>1122391.0</td>\n",
       "      <td>ŋə̀búrŋɛ̀ ánó</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20477.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96433</th>\n",
       "      <td>human_label</td>\n",
       "      <td>1050093.0</td>\n",
       "      <td>1051320.0</td>\n",
       "      <td>lá vŕðìt̪ɔ̀ kukuŋu ðɛdɔɽɔ</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20478.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96434</th>\n",
       "      <td>human_label</td>\n",
       "      <td>1739646.0</td>\n",
       "      <td>1741586.0</td>\n",
       "      <td>ùrnɔ̀ kə̀ŋací áprì kúkùŋ</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>/Users/markjos/Library/CloudStorage/GoogleDriv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20479.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tier_name      start        end  \\\n",
       "75955  human_label  2497647.0  2498747.0   \n",
       "75956  human_label  2325216.0  2326306.0   \n",
       "75957  human_label   354974.0   356382.0   \n",
       "75958  human_label   154905.0   157215.0   \n",
       "75959  human_label  2311065.0  2312385.0   \n",
       "...            ...        ...        ...   \n",
       "96430  human_label  1005320.0  1007400.0   \n",
       "96431  human_label   997005.0   997694.0   \n",
       "96432  human_label  1121405.0  1122391.0   \n",
       "96433  human_label  1050093.0  1051320.0   \n",
       "96434  human_label  1739646.0  1741586.0   \n",
       "\n",
       "                                 transcription  \\\n",
       "75955                   lə̀və̀lɛ̀ðɛ́l únɛ̀ɾɛ̀   \n",
       "75956                              kə̀ŋàcîí   \n",
       "75957            ŋ̀gátɛ́və́lɛ̂ðɔ́ nd̪ɔ̀bàgɛ̀   \n",
       "75958  ŋɔ́ðɔ́ ŋá və́lɛ̀ðà ðàŋàlà nd̪ɔ̀bà   \n",
       "75959                            ðə̀və̀lèðɔ̀ŋ   \n",
       "...                                        ...   \n",
       "96430            ðá nɛ́lê və̀lɛ̀ðɔ́ nd̪ɔ̀bà   \n",
       "96431                                  ŋə̀vrà   \n",
       "96432                         ŋə̀búrŋɛ̀ ánó   \n",
       "96433             lá vŕðìt̪ɔ̀ kukuŋu ðɛdɔɽɔ   \n",
       "96434           ùrnɔ̀ kə̀ŋací áprì kúkùŋ   \n",
       "\n",
       "                                                eaf_path  \\\n",
       "75955  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "75956  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "75957  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "75958  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "75959  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "...                                                  ...   \n",
       "96430  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "96431  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "96432  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "96433  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "96434  /Users/markjos/Library/CloudStorage/GoogleDriv...   \n",
       "\n",
       "                                              wav_source sli_pred  asr_index  \\\n",
       "75955  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN        0.0   \n",
       "75956  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN        1.0   \n",
       "75957  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN        2.0   \n",
       "75958  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN        3.0   \n",
       "75959  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN        4.0   \n",
       "...                                                  ...      ...        ...   \n",
       "96430  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN    20475.0   \n",
       "96431  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN    20476.0   \n",
       "96432  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN    20477.0   \n",
       "96433  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN    20478.0   \n",
       "96434  /Users/markjos/Library/CloudStorage/GoogleDriv...      NaN    20479.0   \n",
       "\n",
       "       split  \n",
       "75955  train  \n",
       "75956   test  \n",
       "75957  train  \n",
       "75958  train  \n",
       "75959  train  \n",
       "...      ...  \n",
       "96430  train  \n",
       "96431   test  \n",
       "96432  train  \n",
       "96433  train  \n",
       "96434  train  \n",
       "\n",
       "[20480 rows x 9 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df['tier_name']=='human_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        HH01082021\n",
       "1        HH01082021\n",
       "2        HH01082021\n",
       "3        HH01082021\n",
       "4        HH01082021\n",
       "            ...    \n",
       "63705    HH20230724\n",
       "63706    HH20230516\n",
       "63707    HH20230626\n",
       "63708    HH20240308\n",
       "63709    HH20210707\n",
       "Name: filestem, Length: 63710, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['filestem']=merged_df['eaf_path'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "merged_df['filestem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make language-balanced splits\n",
    "- Take 2/3 of Tira ASR train data for Tira\n",
    "- Make English dataset of same size\n",
    "- Take remaining 1/3 of Tira ASR dataset and concat English to be same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63480, 10)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df=merged_df.copy()\n",
    "balance_df=balance_df[balance_df['filestem']!='HH20210312']\n",
    "balance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep lists of indices for each data partition\n",
    "tira_mono_indices = []\n",
    "eng_mono_indices = []\n",
    "tira_cs_indices = []\n",
    "tira_cs_indices_20_80_small = []\n",
    "eng_cs_indices_50_50 = []\n",
    "eng_cs_indices_80_20_small = []\n",
    "eng_cs_indices_80_20_large = []\n",
    "get_all_groups = lambda : tira_mono_indices + eng_mono_indices + tira_cs_indices + tira_cs_indices_20_80_small + eng_cs_indices_50_50 + eng_cs_indices_80_20_small + eng_cs_indices_80_20_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_split_duration = lambda df, idcs: df.loc[idcs, 'duration'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Tira btw mono and CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tira_total_duration/(60_000)=561.7255333333334\n",
      "tira_split_duration/60_000=6.3005\n",
      "tira_split_duration/60_000=41.854016666666666\n",
      "tira_split_duration/60_000=76.3835\n",
      "tira_split_duration/60_000=111.21041666666666\n",
      "tira_split_duration/60_000=146.20178333333334\n",
      "tira_split_duration/60_000=180.37508333333332\n",
      "tira_split_duration/60_000=214.38485\n",
      "tira_split_duration/60_000=249.57761666666667\n",
      "tira_split_duration/60_000=284.3081833333333\n",
      "tira_split_duration/60_000=319.1791166666667\n",
      "tira_split_duration/60_000=354.62655\n",
      "len(tira_mono_indices)=10728, len(tira_cs_indices)=5426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_balanced_dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>3.120263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tira_mono</th>\n",
       "      <td>6.241829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       duration\n",
       "lang_balanced_dataset          \n",
       "cs                     3.120263\n",
       "tira_mono              6.241829"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tira_mono_indices = []\n",
    "tira_cs_indices = []\n",
    "tira_asr_train_idcs = balance_df[\n",
    "    (balance_df['tier_name']=='human_label')&\n",
    "    (balance_df['split']=='train')\n",
    "].index\n",
    "# randomly sample 2/3 for Tira lang dataset\n",
    "# initialize w/ Tira samples from recordings w/ no attested English\n",
    "no_eng_files=[]\n",
    "for filestem in balance_df['filestem'].unique():\n",
    "    filestem_mask = balance_df['filestem']==filestem\n",
    "    eng_mask = balance_df['sli_pred']=='ENG'\n",
    "    if (filestem_mask&eng_mask).sum()==0:\n",
    "        no_eng_files.append(filestem)\n",
    "tira_mono_indices.extend(balance_df.loc[balance_df['filestem'].isin(no_eng_files)].index.tolist())\n",
    "\n",
    "balance_df['duration']=balance_df['end']-balance_df['start']\n",
    "tira_total_duration = balance_df.loc[tira_asr_train_idcs, 'duration'].sum()\n",
    "print(f\"{tira_total_duration/(60_000)=}\")\n",
    "tira_split_duration = get_split_duration(balance_df, tira_mono_indices)\n",
    "i=0\n",
    "while tira_split_duration < tira_total_duration*2/3:\n",
    "    if i%1000==0:\n",
    "        print(f\"{tira_split_duration/60_000=}\")\n",
    "    idx = np.random.choice(tira_asr_train_idcs[~tira_asr_train_idcs.isin(tira_mono_indices)])\n",
    "    tira_mono_indices.append(idx)\n",
    "    tira_split_duration = get_split_duration(balance_df, tira_mono_indices)\n",
    "    i+=1\n",
    "tira_cs_indices.extend(balance_df[\n",
    "    (balance_df['tier_name']=='human_label')&\n",
    "    (~balance_df.index.isin(tira_mono_indices))\n",
    "].index)\n",
    "print(f\"{len(tira_mono_indices)=}, {len(tira_cs_indices)=}\")\n",
    "balance_df.loc[tira_mono_indices, 'lang_balanced_dataset']='tira_mono'\n",
    "balance_df.loc[tira_cs_indices, 'lang_balanced_dataset']='cs'\n",
    "balance_df.pivot_table(index='lang_balanced_dataset', values='duration', aggfunc='sum', fill_value=0)/3_600_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/tira_mono_indices.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, tira_mono_indices)))\n",
    "with open('notebooks/longform_dataset/tira_cs_indices.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, tira_cs_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_eng_row(df, idx):\n",
    "    asr_row = df.loc[idx]\n",
    "    filestem_mask = df['filestem']==asr_row['filestem']\n",
    "    eng_mask = df['sli_pred']=='ENG'\n",
    "    ungrouped_mask = ~df.index.isin(get_all_groups())\n",
    "    eng_rows = df[filestem_mask&eng_mask&ungrouped_mask]\n",
    "    if eng_rows.shape[0]==0:\n",
    "        # print(asr_row['filestem'], end=', ')\n",
    "        # print(f\"{filestem_mask.sum()=}\")\n",
    "        # print(f\"{ungrouped_mask.sum()=}\")\n",
    "        # print(f\"{(filestem_mask&ungrouped_mask).sum()=}\")\n",
    "        # print(f\"{(eng_mask&ungrouped_mask).sum()=}\")\n",
    "        # print(f\"{(filestem_mask&eng_mask).sum()=}\")\n",
    "        # print(asr_row)\n",
    "        return\n",
    "    eng_row = eng_rows.iloc[\n",
    "        (eng_rows['start']-asr_row['start']).abs().argsort().iloc[0]\n",
    "    ]\n",
    "    return eng_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get English records for 50/50 Tira/Eng split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_cs_duration=0.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.0\n",
      "eng_cs_duration=2191133.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.1950630760621479\n",
      "eng_cs_duration=4430978.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.3944626814728745\n",
      "eng_cs_duration=6821561.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.6072815626461661\n",
      "eng_cs_duration=9314611.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.829222449747377\n",
      "eng_cs_duration=11860672.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.0558825796901365\n",
      "Looped through dataset, returning to start...\n",
      "nonadjacent_add_ct=471\n"
     ]
    }
   ],
   "source": [
    "eng_cs_indices_50_50 = []\n",
    "asr_idx2cs50_50_idcs = {balance_df.at[idx, 'asr_index']:[idx] for idx in tira_cs_indices}\n",
    "balance_df.loc[(balance_df['tier_name']=='asr'), 'asr_index'] = np.nan\n",
    "\n",
    "tira_cs_duration = get_split_duration(balance_df, tira_cs_indices)\n",
    "eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_50_50)\n",
    "i=-1\n",
    "nonadjacent_add_ct=0\n",
    "looped_once = False\n",
    "while (not looped_once) or (eng_cs_duration < tira_cs_duration):\n",
    "    i+=1\n",
    "    if i==len(tira_cs_indices):\n",
    "        print(\"Looped through dataset, returning to start...\")\n",
    "        looped_once = True\n",
    "        if (eng_cs_duration >= tira_cs_duration):\n",
    "            break\n",
    "        i=0    \n",
    "    if i%1_000==0:\n",
    "        print(f\"{eng_cs_duration=}, {tira_cs_duration=}, {eng_cs_duration/tira_cs_duration=}\")\n",
    "\n",
    "    idx = tira_cs_indices[i]\n",
    "    eng_row = get_nearest_eng_row(balance_df, idx)\n",
    "    if eng_row is None:\n",
    "        eng_mask = balance_df['sli_pred']=='ENG'\n",
    "        ungrouped_mask = ~balance_df.index.isin(get_all_groups())\n",
    "        # eng_row_idx = np.random.choice(balance_df[eng_mask&ungrouped_mask].index)\n",
    "        eng_row_idx = balance_df[eng_mask&ungrouped_mask].sort_values('duration').iloc[0].name\n",
    "        eng_row = balance_df.loc[eng_row_idx]\n",
    "        nonadjacent_add_ct+=1\n",
    "    eng_cs_indices_50_50.append(eng_row.name)\n",
    "    asr_index=balance_df.at[idx, 'asr_index']\n",
    "    asr_idx2cs50_50_idcs[asr_index].append(eng_row.name)\n",
    "    eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_50_50)\n",
    "\n",
    "print(f\"{nonadjacent_add_ct=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tier_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asr</th>\n",
       "      <td>3.600599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human_label</th>\n",
       "      <td>3.120263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             duration\n",
       "tier_name            \n",
       "asr          3.600599\n",
       "human_label  3.120263"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.loc[eng_cs_indices_50_50+tira_cs_indices].pivot_table(index='tier_name', values='duration', aggfunc='sum', fill_value=0)/3_600_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(v) for v in asr_idx2cs50_50_idcs.values())/len(asr_idx2cs50_50_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/eng_cs_indices_50_50.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, eng_cs_indices_50_50)))\n",
    "\n",
    "def prepare_for_json(d):\n",
    "    converted_keys = {int(k):v for k,v in d.items()}\n",
    "    converted_vals = {k:[int(x) for x in v] for k,v in converted_keys.items()}\n",
    "    return converted_vals\n",
    "with open('notebooks/longform_dataset/asr_idx2cs50_50_idcs.json', 'w') as f:\n",
    "    json.dump(prepare_for_json(asr_idx2cs50_50_idcs), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get English records for 20/80 big split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_cs_duration=0.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.0\n",
      "eng_cs_duration=2779019.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.24739894592211162\n",
      "eng_cs_duration=5707132.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.5080708124119888\n",
      "eng_cs_duration=8438281.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=0.7512081870597437\n",
      "eng_cs_duration=11328412.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.0084987500162468\n",
      "eng_cs_duration=14110802.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.25619779530677\n",
      "Looped through dataset, returning to start...\n",
      "eng_cs_duration=15369420.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.368244804168025\n",
      "eng_cs_duration=18092992.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.610707645171623\n",
      "eng_cs_duration=20976328.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=1.8673932911277238\n",
      "eng_cs_duration=23781263.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=2.11709937891627\n",
      "eng_cs_duration=26586942.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=2.3668717004426085\n",
      "eng_cs_duration=29318953.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=2.6100858136414082\n",
      "Looped through dataset, returning to start...\n",
      "eng_cs_duration=30406656.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=2.7069173126978443\n",
      "eng_cs_duration=33175359.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=2.9533978886749743\n",
      "eng_cs_duration=36047121.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=3.2090531726939666\n",
      "eng_cs_duration=38697739.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=3.4450213683925837\n",
      "eng_cs_duration=41378384.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=3.683662682968475\n",
      "eng_cs_duration=44111056.0, tira_cs_duration=11232946.0, eng_cs_duration/tira_cs_duration=3.9269356409262537\n",
      "nonadjacent_add_ct=3987\n"
     ]
    }
   ],
   "source": [
    "eng_cs_indices_80_20_large = []\n",
    "asr_idx2cs20_80_large_idcs = {balance_df.at[idx, 'asr_index']:[idx] for idx in tira_cs_indices}\n",
    "\n",
    "tira_cs_duration = get_split_duration(balance_df, tira_cs_indices)\n",
    "eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_80_20_large)\n",
    "i=-1\n",
    "nonadjacent_add_ct=0\n",
    "looped_once = False\n",
    "while (not looped_once) or (eng_cs_duration < tira_cs_duration*4):\n",
    "    i+=1\n",
    "    if i==len(tira_cs_indices):\n",
    "        print(\"Looped through dataset, returning to start...\")\n",
    "        looped_once = True\n",
    "        i=0    \n",
    "    if i%1_000==0:\n",
    "        print(f\"{eng_cs_duration=}, {tira_cs_duration=}, {eng_cs_duration/tira_cs_duration=}\")\n",
    "\n",
    "    idx = tira_cs_indices[i]\n",
    "    eng_row = get_nearest_eng_row(balance_df, idx)\n",
    "    if eng_row is None:\n",
    "        eng_mask = balance_df['sli_pred']=='ENG'\n",
    "        ungrouped_mask = ~balance_df.index.isin(eng_cs_indices_80_20_large)\n",
    "        eng_row_idx = np.random.choice(balance_df[eng_mask&ungrouped_mask].index)\n",
    "        # eng_row_idx = balance_df[eng_mask&ungrouped_mask].sort_values('duration').iloc[0].name\n",
    "        eng_row = balance_df.loc[eng_row_idx]\n",
    "        nonadjacent_add_ct+=1\n",
    "    eng_cs_indices_80_20_large.append(eng_row.name)\n",
    "    asr_index=balance_df.at[idx, 'asr_index']\n",
    "    asr_idx2cs20_80_large_idcs[asr_index].append(eng_row.name)\n",
    "    eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_80_20_large)\n",
    "print(f\"{nonadjacent_add_ct=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tier_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asr</th>\n",
       "      <td>12.481507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human_label</th>\n",
       "      <td>3.120263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              duration\n",
       "tier_name             \n",
       "asr          12.481507\n",
       "human_label   3.120263"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.loc[eng_cs_indices_80_20_large+tira_cs_indices].pivot_table(index='tier_name', values='duration', aggfunc='sum', fill_value=0)/3_600_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/eng_cs_indices_80_20_large.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, eng_cs_indices_80_20_large)))\n",
    "with open('notebooks/longform_dataset/asr_idx2cs20_80_large_idcs.json', 'w') as f:\n",
    "    json.dump(prepare_for_json(asr_idx2cs20_80_large_idcs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9799115370438627"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(v) for v in asr_idx2cs20_80_large_idcs.values())/len(asr_idx2cs20_80_large_idcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tira records for 20/80 small split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tira_cs_indices_20_80_small)=2154\n"
     ]
    }
   ],
   "source": [
    "tira_cs_indices_20_80_small = []\n",
    "valid_idcs = tira_cs_indices.copy()\n",
    "shuffle(valid_idcs)\n",
    "\n",
    "tira_mono_duration = get_split_duration(balance_df, tira_mono_indices)\n",
    "while get_split_duration(balance_df, tira_cs_indices_20_80_small) < tira_mono_duration*0.2:\n",
    "    idx = valid_idcs.pop()\n",
    "    tira_cs_indices_20_80_small.append(idx)\n",
    "print(f\"{len(tira_cs_indices_20_80_small)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/tira_cs_indices_20_80_small.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, tira_cs_indices_20_80_small)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get English records for 20/80 small split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_cs_duration=0.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=0.0\n",
      "eng_cs_duration=2726570.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=0.6066781613261891\n",
      "eng_cs_duration=5436288.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=1.2096066516831132\n",
      "Looped through dataset, returning to start...\n",
      "eng_cs_duration=5844072.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=1.300341034933218\n",
      "eng_cs_duration=8606616.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=1.9150236267987106\n",
      "eng_cs_duration=11437238.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=2.544853981555588\n",
      "Looped through dataset, returning to start...\n",
      "eng_cs_duration=11846520.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=2.6359216787810054\n",
      "eng_cs_duration=14557304.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=3.2390873605249006\n",
      "eng_cs_duration=17211749.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=3.8297172772119823\n",
      "Looped through dataset, returning to start...\n",
      "eng_cs_duration=17602725.0, tira_cs_duration=4494261.0, eng_cs_duration/tira_cs_duration=3.9167117797564495\n",
      "nonadjacent_add_ct=3119\n"
     ]
    }
   ],
   "source": [
    "eng_cs_indices_80_20_small = []\n",
    "asr_idx2cs20_80_small_idcs = {balance_df.at[idx, 'asr_index']:[idx] for idx in tira_cs_indices_20_80_small}\n",
    "\n",
    "tira_cs_duration = get_split_duration(balance_df, tira_cs_indices_20_80_small)\n",
    "eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_80_20_small)\n",
    "i=-1\n",
    "nonadjacent_add_ct=0\n",
    "looped_once = False\n",
    "while (not looped_once) or (eng_cs_duration < tira_cs_duration*4):\n",
    "    i+=1\n",
    "    if i==len(tira_cs_indices_20_80_small):\n",
    "        print(\"Looped through dataset, returning to start...\")\n",
    "        looped_once = True\n",
    "        i=0    \n",
    "    if i%1_000==0:\n",
    "        print(f\"{eng_cs_duration=}, {tira_cs_duration=}, {eng_cs_duration/tira_cs_duration=}\")\n",
    "\n",
    "    idx = tira_cs_indices_20_80_small[i]\n",
    "    eng_row = get_nearest_eng_row(balance_df, idx)\n",
    "    if eng_row is None:\n",
    "        eng_mask = balance_df['sli_pred']=='ENG'\n",
    "        ungrouped_mask = ~balance_df.index.isin(eng_cs_indices_80_20_small)\n",
    "        eng_row_idx = np.random.choice(balance_df[eng_mask&ungrouped_mask].index)\n",
    "        # eng_row_idx = balance_df[eng_mask&ungrouped_mask].sort_values('duration').iloc[0].name\n",
    "        eng_row = balance_df.loc[eng_row_idx]\n",
    "        nonadjacent_add_ct+=1\n",
    "    eng_cs_indices_80_20_small.append(eng_row.name)\n",
    "    asr_index=balance_df.at[idx, 'asr_index']\n",
    "    asr_idx2cs20_80_small_idcs[asr_index].append(eng_row.name)\n",
    "    eng_cs_duration = get_split_duration(balance_df, eng_cs_indices_80_20_small)\n",
    "print(f\"{nonadjacent_add_ct=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tier_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asr</th>\n",
       "      <td>4.993824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human_label</th>\n",
       "      <td>1.248406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             duration\n",
       "tier_name            \n",
       "asr          4.993824\n",
       "human_label  1.248406"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.loc[eng_cs_indices_80_20_small+tira_cs_indices_20_80_small].pivot_table(index='tier_name', values='duration', aggfunc='sum', fill_value=0)/3_600_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.058960074280408"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(v) for v in asr_idx2cs20_80_small_idcs.values())/len(asr_idx2cs20_80_small_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/eng_cs_indices_80_20_small.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, eng_cs_indices_80_20_small)))\n",
    "\n",
    "with open('notebooks/longform_dataset/asr_idx2cs20_80_small_idcs.json', 'w') as f:\n",
    "    json.dump(prepare_for_json(asr_idx2cs20_80_small_idcs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.68642861111111"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split_duration(balance_df, get_all_groups())/3_600_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get English records for English monolingual partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(eng_mono_indices)=8592\n"
     ]
    }
   ],
   "source": [
    "eng_mono_indices = []\n",
    "valid_idcs = balance_df[\n",
    "    (balance_df['sli_pred']=='ENG')&\n",
    "    (~balance_df.index.isin(get_all_groups()))\n",
    "].index.to_list()\n",
    "shuffle(valid_idcs)\n",
    "while get_split_duration(balance_df, eng_mono_indices) < tira_mono_duration:\n",
    "    idx = valid_idcs.pop()\n",
    "    eng_mono_indices.append(idx)\n",
    "print(f\"{len(eng_mono_indices)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/longform_dataset/eng_mono_indices.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, eng_mono_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df_path = 'notebooks/longform_dataset/balance_df.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does normalization do to Tira text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tira_transcriptions = balance_df.loc[tira_mono_indices, 'transcription'].to_list()\n",
    "nmzd=[tokenizer.normalize(t) for t in tira_transcriptions]\n",
    "bsc_nmzd=[tokenizer.basic_normalize(t) for t in tira_transcriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>normalized</th>\n",
       "      <th>basic_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ìŋgádɔ̀ ɔ́ɾɛ́</td>\n",
       "      <td>iŋgadɔ ɔɾɛ</td>\n",
       "      <td>ìŋgádɔ ɔ ɾɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kàðɛ́və̀lèðɔ́</td>\n",
       "      <td>kadɛvəledɔ</td>\n",
       "      <td>kàðɛ və lèðɔ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ŋàðɛ́ úrnɔ̀ və̀lèðɔ́ t̪àwə̀nì</td>\n",
       "      <td>ŋadɛ urnɔ vəledɔ tawəni</td>\n",
       "      <td>ŋàðɛ úrnɔ və lèðɔ t àwə nì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ŋàðɛ́ úrnɔ̀ və̀lèðà</td>\n",
       "      <td>ŋadɛ urnɔ vəleda</td>\n",
       "      <td>ŋàðɛ úrnɔ və lèðà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kàðɛ́və̀lèðà ŋìnɛ̀</td>\n",
       "      <td>kadɛvəleda ŋinɛ</td>\n",
       "      <td>kàðɛ və lèðà ŋìnɛ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        transcription               normalized  \\\n",
       "0                     ìŋgádɔ̀ ɔ́ɾɛ́               iŋgadɔ ɔɾɛ   \n",
       "1                     kàðɛ́və̀lèðɔ́               kadɛvəledɔ   \n",
       "2  ŋàðɛ́ úrnɔ̀ və̀lèðɔ́ t̪àwə̀nì  ŋadɛ urnɔ vəledɔ tawəni   \n",
       "3             ŋàðɛ́ úrnɔ̀ və̀lèðà         ŋadɛ urnɔ vəleda   \n",
       "4              kàðɛ́və̀lèðà ŋìnɛ̀          kadɛvəleda ŋinɛ   \n",
       "\n",
       "             basic_normalized  \n",
       "0                ìŋgádɔ ɔ ɾɛ   \n",
       "1               kàðɛ və lèðɔ   \n",
       "2  ŋàðɛ úrnɔ və lèðɔ t àwə nì  \n",
       "3           ŋàðɛ úrnɔ və lèðà  \n",
       "4          kàðɛ və lèðà ŋìnɛ   "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmz_df = pd.DataFrame({'transcription':tira_transcriptions, 'normalized':nmzd, 'basic_normalized':bsc_nmzd})\n",
    "nmz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df.index.name='index'\n",
    "balance_df.to_csv(balance_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make text files for training LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tier_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>eaf_path</th>\n",
       "      <th>wav_source</th>\n",
       "      <th>sli_pred</th>\n",
       "      <th>asr_index</th>\n",
       "      <th>split</th>\n",
       "      <th>filestem</th>\n",
       "      <th>duration</th>\n",
       "      <th>lang_balanced_dataset</th>\n",
       "      <th>label_path</th>\n",
       "      <th>clip_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>asr</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1414.0</td>\n",
       "      <td>hello hello hello</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH01082021</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/elicitation-wavs/wav/labels/0.txt</td>\n",
       "      <td>data/elicitation-wavs/wav/clips/0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>asr</td>\n",
       "      <td>6240.0</td>\n",
       "      <td>8502.0</td>\n",
       "      <td>hello hello this shit</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH01082021</td>\n",
       "      <td>2262.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/elicitation-wavs/wav/labels/1.txt</td>\n",
       "      <td>data/elicitation-wavs/wav/clips/1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>asr</td>\n",
       "      <td>11387.0</td>\n",
       "      <td>13244.0</td>\n",
       "      <td>i do not know</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH01082021</td>\n",
       "      <td>1857.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/elicitation-wavs/wav/labels/2.txt</td>\n",
       "      <td>data/elicitation-wavs/wav/clips/2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>asr</td>\n",
       "      <td>13649.0</td>\n",
       "      <td>13986.0</td>\n",
       "      <td>yeah</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH01082021</td>\n",
       "      <td>337.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/elicitation-wavs/wav/labels/3.txt</td>\n",
       "      <td>data/elicitation-wavs/wav/clips/3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>asr</td>\n",
       "      <td>26625.0</td>\n",
       "      <td>27064.0</td>\n",
       "      <td>thank you</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/a...</td>\n",
       "      <td>/home/AD/mjsimmons/datasets/elicitation-wavs/m...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HH01082021</td>\n",
       "      <td>439.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/elicitation-wavs/wav/labels/4.txt</td>\n",
       "      <td>data/elicitation-wavs/wav/clips/4.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 tier_name    start      end          transcription  \\\n",
       "index                                                                  \n",
       "0               0       asr     30.0   1414.0      hello hello hello   \n",
       "1               1       asr   6240.0   8502.0  hello hello this shit   \n",
       "2               2       asr  11387.0  13244.0          i do not know   \n",
       "3               3       asr  13649.0  13986.0                   yeah   \n",
       "4               4       asr  26625.0  27064.0              thank you   \n",
       "\n",
       "                                                eaf_path  \\\n",
       "index                                                      \n",
       "0      /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "1      /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "2      /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "3      /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "4      /home/AD/mjsimmons/datasets/elicitation-wavs/a...   \n",
       "\n",
       "                                              wav_source sli_pred  asr_index  \\\n",
       "index                                                                          \n",
       "0      /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   \n",
       "1      /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   \n",
       "2      /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   \n",
       "3      /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   \n",
       "4      /home/AD/mjsimmons/datasets/elicitation-wavs/m...      ENG        NaN   \n",
       "\n",
       "      split    filestem  duration lang_balanced_dataset  \\\n",
       "index                                                     \n",
       "0       NaN  HH01082021    1384.0                   NaN   \n",
       "1       NaN  HH01082021    2262.0                   NaN   \n",
       "2       NaN  HH01082021    1857.0                   NaN   \n",
       "3       NaN  HH01082021     337.0                   NaN   \n",
       "4       NaN  HH01082021     439.0                   NaN   \n",
       "\n",
       "                                   label_path  \\\n",
       "index                                           \n",
       "0      data/elicitation-wavs/wav/labels/0.txt   \n",
       "1      data/elicitation-wavs/wav/labels/1.txt   \n",
       "2      data/elicitation-wavs/wav/labels/2.txt   \n",
       "3      data/elicitation-wavs/wav/labels/3.txt   \n",
       "4      data/elicitation-wavs/wav/labels/4.txt   \n",
       "\n",
       "                                   clip_name  \n",
       "index                                         \n",
       "0      data/elicitation-wavs/wav/clips/0.wav  \n",
       "1      data/elicitation-wavs/wav/clips/1.wav  \n",
       "2      data/elicitation-wavs/wav/clips/2.wav  \n",
       "3      data/elicitation-wavs/wav/clips/3.wav  \n",
       "4      data/elicitation-wavs/wav/clips/4.wav  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df=pd.read_csv(balance_df_path, index_col='index')\n",
    "balance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47326/47326 [00:05<00:00, 7929.08it/s]\n",
      "100%|██████████| 47326/47326 [00:00<00:00, 374068.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['675 1913 7751 7751\\n',\n",
       " '675 1913 7751 341 4611\\n',\n",
       " '72 360 406 458\\n',\n",
       " '19650\\n',\n",
       " '40683 291\\n']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sent=balance_df.loc[balance_df['sli_pred']=='ENG', 'transcription'].unique()\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-tiny')\n",
    "token_lists=[\n",
    "    tokenizer.encode(sentence, add_special_tokens=False) for sentence in tqdm(unique_sent)\n",
    "]\n",
    "token_sents = [' '.join(str(t) for t in sent)+'\\n' for sent in tqdm(token_lists)]\n",
    "token_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/markjos/projects/malachor5/data/hf-datasets/eng_mono/eng_tkzd.txt', 'w') as f:\n",
    "    f.writelines(token_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_to_frames = lambda ms: int(ms*16)\n",
    "\n",
    "elicitation_wav_dir = 'data/elicitation-wavs/wav'\n",
    "tira_cs_dir = 'data/hf-datasets/tira_cs_balanced'\n",
    "os.makedirs(os.path.join(tira_cs_dir, 'train'), exist_ok=True)\n",
    "\n",
    "def snip_record(row):\n",
    "    wav_path = os.path.join(\n",
    "        elicitation_wav_dir,\n",
    "        os.path.basename(row['wav_source'])\n",
    "    )\n",
    "    wav = load_and_resample(wav_path)\n",
    "    start_frame = ms_to_frames(row['start'])\n",
    "    end_frame = ms_to_frames(row['end'])\n",
    "    snip = wav[start_frame:end_frame]\n",
    "    del wav\n",
    "    return snip\n",
    "\n",
    "def concat_recordings_and_save(rows, i):\n",
    "    clip_basename = str(i)+'.wav'\n",
    "    clip_relpath = os.path.join('train', clip_basename)\n",
    "    clip_path = os.path.join(tira_cs_dir, clip_relpath)\n",
    "    snips = rows.apply(snip_record, axis=1).tolist()\n",
    "    torchaudio.save(clip_path, torch.concatenate(snips), 16_000)\n",
    "    return clip_relpath\n",
    "\n",
    "def make_cs_labels(df):\n",
    "    cs_labels = []\n",
    "    for i in tqdm(df['asr_index'].unique().tolist()):\n",
    "        if np.isnan(i):\n",
    "            continue\n",
    "        i=int(i)\n",
    "        has_index = df['asr_index']==i\n",
    "        min_start = df[has_index]['start'].min()\n",
    "        max_end = df[has_index]['end'].max()\n",
    "        transcription = ' '.join([\n",
    "            s.strip() for s in df[has_index].sort_values('start')['transcription']\n",
    "        ]).strip()\n",
    "        filestem = df[has_index]['filestem'].unique()\n",
    "        clip_relpath = concat_recordings_and_save(df[has_index], i)\n",
    "        cs_labels.append({\n",
    "            'asr_index': i,\n",
    "            'start': min_start,\n",
    "            'end': max_end,\n",
    "            'transcription': transcription,\n",
    "            'indices': df[has_index].sort_values('start').index.tolist(),\n",
    "            'duration': max_end-min_start,\n",
    "            'split': 'train',\n",
    "            'filestem': filestem,\n",
    "            'file_name': clip_relpath,\n",
    "        })\n",
    "    return pd.DataFrame(cs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cs_labels(balance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_balanced_dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>22444567.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tira_mono</th>\n",
       "      <td>22470586.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         duration\n",
       "lang_balanced_dataset            \n",
       "cs                     22444567.0\n",
       "tira_mono              22470586.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.pivot_table(index='lang_balanced_dataset', values='duration', aggfunc='sum', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_split_duration=0.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.0\n",
      "eng_split_duration=2789629.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.12415401595969576\n",
      "eng_split_duration=5552224.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.2471048684638014\n",
      "eng_split_duration=5552224.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.2471048684638014\n",
      "eng_split_duration=5552224.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.2471048684638014\n",
      "eng_split_duration=5552224.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.2471048684638014\n",
      "eng_split_duration=8267010.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.36792795439069653\n",
      "eng_split_duration=8267010.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.36792795439069653\n",
      "eng_split_duration=11049665.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.4917715885371466\n",
      "eng_split_duration=11049665.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.4917715885371466\n",
      "eng_split_duration=13865851.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.6171075387977266\n",
      "eng_split_duration=16530655.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.7357061475537516\n",
      "eng_split_duration=19328502.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.8602259102500768\n",
      "eng_split_duration=22022073.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.9801048106065664\n",
      "eng_split_duration=22022073.0 tira_split_duration=22469100.0 eng_split_duration/tira_split_duration=0.9801048106065664\n",
      "lang_balanced_dataset\n",
      "cs      11061\n",
      "tira    10746\n",
      "eng      8179\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_balanced_dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>22593688.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>22469248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tira</th>\n",
       "      <td>22469100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         duration\n",
       "lang_balanced_dataset            \n",
       "cs                     22593688.0\n",
       "eng                    22469248.0\n",
       "tira                   22469100.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.loc[\n",
    "    ~balance_df['lang_balanced_dataset'].isin(['cs', 'tira']),\n",
    "    'lang_balanced_dataset'\n",
    "]=np.nan\n",
    "# randomly sample 2/3 for Tira lang dataset\n",
    "# initialize w/ Tira samples from recordings w/ no attested English\n",
    "\n",
    "tira_split_duration = get_tira_split_duration(balance_df)\n",
    "eng_split_duration = get_eng_split_duration(balance_df)\n",
    "eng_mask = balance_df['sli_pred']=='ENG'\n",
    "i=0\n",
    "while eng_split_duration < tira_split_duration:\n",
    "    if i%1000==0:\n",
    "        print(f\"{eng_split_duration=} {tira_split_duration=} {eng_split_duration/tira_split_duration=}\")\n",
    "    ungrouped_mask = balance_df['lang_balanced_dataset'].isna()\n",
    "    idx = np.random.choice(\n",
    "        balance_df[ungrouped_mask&eng_mask].index\n",
    "    )\n",
    "    # skip recordings with just one word\n",
    "    if len(balance_df.loc[idx, 'transcription'].split())<=1:\n",
    "        continue\n",
    "    balance_df.loc[idx, 'lang_balanced_dataset']='eng'\n",
    "    balance_df.loc[idx, 'split']='train'\n",
    "    eng_split_duration = get_eng_split_duration(balance_df)\n",
    "    i+=1\n",
    "print(balance_df['lang_balanced_dataset'].value_counts())\n",
    "balance_df.pivot_table(index='lang_balanced_dataset', values='duration', aggfunc='sum', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_balanced_dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>6.276024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>6.241458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tira</th>\n",
       "      <td>6.241417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       duration\n",
       "lang_balanced_dataset          \n",
       "cs                     6.276024\n",
       "eng                    6.241458\n",
       "tira                   6.241417"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_df.pivot_table(index='lang_balanced_dataset', values='duration', aggfunc='sum', fill_value=0)/(60*60*1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group annotations\n",
    "- For each recording\n",
    "  - Sort labels by start time\n",
    "  - For each human label in recording\n",
    "    - Assign rows to the label by associating preceding or following machine labels to the human label\n",
    "    - Associate rows by setting the `asr_index` col for each machine label to the matching human label\n",
    "    - Stop when adding another label would make the label >30s or when another `in_dataset` or `human_label` row is reached\n",
    "    - Whenever a label is added, set the column for `in_dataset` to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_machine_labels(df: pd.DataFrame):\n",
    "        df=df.sort_values('start')\n",
    "        def map_asr_indices(row: pd.Series):\n",
    "            start = row['start']\n",
    "            end = row['end']\n",
    "            \n",
    "            add_direction: Literal['prev', 'next'] = 'prev'\n",
    "            last_index_added = None\n",
    "            # while there are still rows in the asr labels that have not been associated with a human label\n",
    "            # and the time window is less than 30 seconds\n",
    "            # associate last index added with human label\n",
    "            while (end-start) < 30_000:\n",
    "                if last_index_added is not None:\n",
    "                    df.at[last_index_added, 'asr_index'] = row['asr_index']\n",
    "                    df.at[last_index_added, 'split'] = row['split']\n",
    "\n",
    "                prev_rows = df[df['end']<=start]\n",
    "                next_rows = df[df['start']>=end]\n",
    "                if len(prev_rows)==0 or not np.isnan(prev_rows.iloc[-1]['asr_index']):\n",
    "                    prev_rows=None\n",
    "                if len(next_rows)==0 or not np.isnan(next_rows.iloc[0]['asr_index']):\n",
    "                    next_rows=None\n",
    "                if (prev_rows is None) and (next_rows is None):\n",
    "                    break\n",
    "                if next_rows is not None and (add_direction == 'next' or prev_rows is None):\n",
    "                    end = next_rows.iloc[0]['end']\n",
    "                    add_direction = 'prev'\n",
    "                    last_index_added = next_rows.iloc[0].name\n",
    "                else: # next_rows is None or add_direction == 'prev':\n",
    "                    start = prev_rows.iloc[-1]['start']\n",
    "                    add_direction = 'next'\n",
    "                    last_index_added = prev_rows.iloc[-1].name\n",
    "        df[df['tier_name']=='human_label'].apply(map_asr_indices, axis=1)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [01:10<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_df['prev_human_label']=np.nan\n",
    "merged_df['next_human_label']=np.nan\n",
    "for file in tqdm(merged_df['filestem'].unique().tolist()):\n",
    "    is_file = merged_df['filestem']==file\n",
    "    merged_df.loc[is_file]=associate_machine_labels(merged_df[is_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71188,\n",
       " 51242,\n",
       " tier_name\n",
       " asr            101950\n",
       " human_label     20480\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~merged_df['asr_index'].isna()).sum(), (merged_df['asr_index'].isna()).sum(), merged_df['tier_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_long_labels(df):\n",
    "    long_labels = []\n",
    "    for i in tqdm(df['asr_index'].unique().tolist()):\n",
    "        if np.isnan(i):\n",
    "            continue\n",
    "        has_index = df['asr_index']==i\n",
    "        min_start = df[has_index]['start'].min()\n",
    "        max_end = df[has_index]['end'].max()\n",
    "        transcription = ' '.join([\n",
    "            s.strip() for s in df[has_index].sort_values('start')['transcription']\n",
    "        ]).strip()\n",
    "        filestem = df[has_index]['filestem'].iloc[0]\n",
    "        split = df[has_index]['split'].iloc[0]\n",
    "        long_labels.append({\n",
    "            'asr_index': i,\n",
    "            'start': min_start,\n",
    "            'end': max_end,\n",
    "            'transcription': transcription,\n",
    "            'indices': df[has_index].sort_values('start').index.tolist(),\n",
    "            'duration': max_end-min_start,\n",
    "            'split': split,\n",
    "            'filestem': filestem,\n",
    "        })\n",
    "    return pd.DataFrame(long_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20481/20481 [00:53<00:00, 383.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_labels_df.shape=(20480, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asr_index</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>indices</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "      <th>filestem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11230.0</td>\n",
       "      <td>1229590.0</td>\n",
       "      <td>1259547.0</td>\n",
       "      <td>íŋgánɔ̀nà ɛ́léɲé kə́ náɾùwè nd̪ɔ̀bà T...</td>\n",
       "      <td>[113180, 69522, 101459, 10, 36988, 69525, 7323...</td>\n",
       "      <td>29957.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20220719-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14569.0</td>\n",
       "      <td>902320.0</td>\n",
       "      <td>930910.0</td>\n",
       "      <td>weird tone patterns. And so that might either ...</td>\n",
       "      <td>[42, 116519]</td>\n",
       "      <td>28590.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20230414-Zoom-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14538.0</td>\n",
       "      <td>1318720.0</td>\n",
       "      <td>1344900.0</td>\n",
       "      <td>làdɔ́ŋnɛ̀ nìðìnɔ́ŋù ùnɛ̀ɾɛ̀ So actually, ...</td>\n",
       "      <td>[116488, 59]</td>\n",
       "      <td>26180.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20220629-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12250.0</td>\n",
       "      <td>3154322.0</td>\n",
       "      <td>3184300.0</td>\n",
       "      <td>from the Karcha, Karcha is actually far away. ...</td>\n",
       "      <td>[31526, 72251, 64, 49843, 37711, 19284, 70756,...</td>\n",
       "      <td>29978.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20221127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6154.0</td>\n",
       "      <td>1148379.0</td>\n",
       "      <td>1177450.0</td>\n",
       "      <td>íŋgáðə́rɔ̀ðà You know, our friend, El Yasse...</td>\n",
       "      <td>[108104, 86, 9008, 63666, 48904]</td>\n",
       "      <td>29071.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH07242020-Zoom2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   asr_index      start        end  \\\n",
       "0    11230.0  1229590.0  1259547.0   \n",
       "1    14569.0   902320.0   930910.0   \n",
       "2    14538.0  1318720.0  1344900.0   \n",
       "3    12250.0  3154322.0  3184300.0   \n",
       "4     6154.0  1148379.0  1177450.0   \n",
       "\n",
       "                                       transcription  \\\n",
       "0  íŋgánɔ̀nà ɛ́léɲé kə́ náɾùwè nd̪ɔ̀bà T...   \n",
       "1  weird tone patterns. And so that might either ...   \n",
       "2  làdɔ́ŋnɛ̀ nìðìnɔ́ŋù ùnɛ̀ɾɛ̀ So actually, ...   \n",
       "3  from the Karcha, Karcha is actually far away. ...   \n",
       "4  íŋgáðə́rɔ̀ðà You know, our friend, El Yasse...   \n",
       "\n",
       "                                             indices  duration  split  \\\n",
       "0  [113180, 69522, 101459, 10, 36988, 69525, 7323...   29957.0  train   \n",
       "1                                       [42, 116519]   28590.0  train   \n",
       "2                                       [116488, 59]   26180.0  train   \n",
       "3  [31526, 72251, 64, 49843, 37711, 19284, 70756,...   29978.0  train   \n",
       "4                   [108104, 86, 9008, 63666, 48904]   29071.0  train   \n",
       "\n",
       "            filestem  \n",
       "0       HH20220719-2  \n",
       "1  HH20230414-Zoom-3  \n",
       "2       HH20220629-2  \n",
       "3         HH20221127  \n",
       "4   HH07242020-Zoom2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_labels_df = make_long_labels(merged_df)\n",
    "print(f\"{long_labels_df.shape=}\")\n",
    "long_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg duration: 11.96s\n",
      "total duration: 68.06hr\n",
      "total duration train: 52.17hr\n",
      "avg duration train: 11.46s\n",
      "total duration validation: 7.97hr\n",
      "avg duration validation: 14.02s\n",
      "total duration test: 7.92hr\n",
      "avg duration test: 13.91s\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg duration: {long_labels_df['duration'].mean()/1_000:.2f}s\")\n",
    "print(f\"total duration: {long_labels_df['duration'].sum()/(1_000*60*60):.2f}hr\")\n",
    "for split in long_labels_df['split'].unique():\n",
    "    split_df = long_labels_df[long_labels_df['split']==split]\n",
    "    print(f\"total duration {split}: {split_df['duration'].sum()/(1_000*60*60):.2f}hr\")    \n",
    "    print(f\"avg duration {split}: {split_df['duration'].mean()/1_000:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_labels_df['duration'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elicitation_wavs = glob('data/elicitation-wavs/wav/*')\n",
    "elicitation_eafs = glob('data/elicitation-wavs/autotranscribed/*.eaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filestem_to_paths = {}\n",
    "for wav in elicitation_wavs:\n",
    "    filestem = os.path.splitext(os.path.basename(wav))[0]\n",
    "    filestem_to_paths[filestem] = {'wav': wav} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wav_with_filestem = lambda fs: filestem_to_paths[fs]['wav']\n",
    "get_eaf_with_filestem = lambda fs: filestem_to_paths[fs]['eaf']\n",
    "long_labels_df['wav_source'] = long_labels_df['filestem'].apply(get_wav_with_filestem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_labels_df.to_csv('data/elicitation-wavs/long_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long labels HF dataset\n",
    "Make a new folder, save `long_labels_df`, snip and move all wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Users/markjos/projects/malachor5/data/elicitation-wavs/wav'\n",
    "long_labels_df['wav_source_mac'] = long_labels_df['wav_source'].apply(lambda x: x.replace(\n",
    "    '/home/AD/mjsimmons/datasets/elicitation-wavs/masked/',\n",
    "    '/Users/markjos/projects/malachor5/data/elicitation-wavs/wav/'\n",
    "))\n",
    "for wav_path in long_labels_df['wav_source_mac'].unique():\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asr_index</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>transcription</th>\n",
       "      <th>indices</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "      <th>filestem</th>\n",
       "      <th>wav_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11230.0</td>\n",
       "      <td>1229590.0</td>\n",
       "      <td>1259547.0</td>\n",
       "      <td>íŋgánɔ̀nà ɛ́léɲé kə́ náɾùwè nd̪ɔ̀bà T...</td>\n",
       "      <td>[113180, 69522, 101459, 10, 36988, 69525, 7323...</td>\n",
       "      <td>29957.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20220719-2</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20220719-2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14569.0</td>\n",
       "      <td>902320.0</td>\n",
       "      <td>930910.0</td>\n",
       "      <td>weird tone patterns. And so that might either ...</td>\n",
       "      <td>[42, 116519]</td>\n",
       "      <td>28590.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20230414-Zoom-3</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20230414-Zoom-3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14538.0</td>\n",
       "      <td>1318720.0</td>\n",
       "      <td>1344900.0</td>\n",
       "      <td>làdɔ́ŋnɛ̀ nìðìnɔ́ŋù ùnɛ̀ɾɛ̀ So actually, ...</td>\n",
       "      <td>[116488, 59]</td>\n",
       "      <td>26180.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20220629-2</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20220629-2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12250.0</td>\n",
       "      <td>3154322.0</td>\n",
       "      <td>3184300.0</td>\n",
       "      <td>from the Karcha, Karcha is actually far away. ...</td>\n",
       "      <td>[31526, 72251, 64, 49843, 37711, 19284, 70756,...</td>\n",
       "      <td>29978.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20221127</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20221127.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6154.0</td>\n",
       "      <td>1148379.0</td>\n",
       "      <td>1177450.0</td>\n",
       "      <td>íŋgáðə́rɔ̀ðà You know, our friend, El Yasse...</td>\n",
       "      <td>[108104, 86, 9008, 63666, 48904]</td>\n",
       "      <td>29071.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH07242020-Zoom2</td>\n",
       "      <td>data/elicitation-wavs/wav/HH07242020-Zoom2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20475</th>\n",
       "      <td>20470.0</td>\n",
       "      <td>1256526.0</td>\n",
       "      <td>1257592.0</td>\n",
       "      <td>ŋàmɽárè</td>\n",
       "      <td>[122420]</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>test</td>\n",
       "      <td>HH08212020-2</td>\n",
       "      <td>data/elicitation-wavs/wav/HH08212020-2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>20471.0</td>\n",
       "      <td>1261946.0</td>\n",
       "      <td>1262962.0</td>\n",
       "      <td>ŋə̀mìɲàt̪ɔ́ ásá</td>\n",
       "      <td>[122421]</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>test</td>\n",
       "      <td>HH20211207-Zoom</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20211207-Zoom.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20477</th>\n",
       "      <td>20475.0</td>\n",
       "      <td>1005320.0</td>\n",
       "      <td>1007400.0</td>\n",
       "      <td>ðá nɛ́lê və̀lɛ̀ðɔ́ nd̪ɔ̀bà</td>\n",
       "      <td>[122425]</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20230516</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20230516.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20478</th>\n",
       "      <td>20477.0</td>\n",
       "      <td>1121405.0</td>\n",
       "      <td>1122391.0</td>\n",
       "      <td>ŋə̀búrŋɛ̀ ánó</td>\n",
       "      <td>[122427]</td>\n",
       "      <td>986.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20230626</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20230626.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20479</th>\n",
       "      <td>20479.0</td>\n",
       "      <td>1739646.0</td>\n",
       "      <td>1741586.0</td>\n",
       "      <td>ùrnɔ̀ kə̀ŋací áprì kúkùŋ</td>\n",
       "      <td>[122429]</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>train</td>\n",
       "      <td>HH20210707</td>\n",
       "      <td>data/elicitation-wavs/wav/HH20210707.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       asr_index      start        end  \\\n",
       "0        11230.0  1229590.0  1259547.0   \n",
       "1        14569.0   902320.0   930910.0   \n",
       "2        14538.0  1318720.0  1344900.0   \n",
       "3        12250.0  3154322.0  3184300.0   \n",
       "4         6154.0  1148379.0  1177450.0   \n",
       "...          ...        ...        ...   \n",
       "20475    20470.0  1256526.0  1257592.0   \n",
       "20476    20471.0  1261946.0  1262962.0   \n",
       "20477    20475.0  1005320.0  1007400.0   \n",
       "20478    20477.0  1121405.0  1122391.0   \n",
       "20479    20479.0  1739646.0  1741586.0   \n",
       "\n",
       "                                           transcription  \\\n",
       "0      íŋgánɔ̀nà ɛ́léɲé kə́ náɾùwè nd̪ɔ̀bà T...   \n",
       "1      weird tone patterns. And so that might either ...   \n",
       "2      làdɔ́ŋnɛ̀ nìðìnɔ́ŋù ùnɛ̀ɾɛ̀ So actually, ...   \n",
       "3      from the Karcha, Karcha is actually far away. ...   \n",
       "4      íŋgáðə́rɔ̀ðà You know, our friend, El Yasse...   \n",
       "...                                                  ...   \n",
       "20475                                         ŋàmɽárè   \n",
       "20476                                ŋə̀mìɲàt̪ɔ́ ásá   \n",
       "20477                      ðá nɛ́lê və̀lɛ̀ðɔ́ nd̪ɔ̀bà   \n",
       "20478                                   ŋə̀búrŋɛ̀ ánó   \n",
       "20479                     ùrnɔ̀ kə̀ŋací áprì kúkùŋ   \n",
       "\n",
       "                                                 indices  duration  split  \\\n",
       "0      [113180, 69522, 101459, 10, 36988, 69525, 7323...   29957.0  train   \n",
       "1                                           [42, 116519]   28590.0  train   \n",
       "2                                           [116488, 59]   26180.0  train   \n",
       "3      [31526, 72251, 64, 49843, 37711, 19284, 70756,...   29978.0  train   \n",
       "4                       [108104, 86, 9008, 63666, 48904]   29071.0  train   \n",
       "...                                                  ...       ...    ...   \n",
       "20475                                           [122420]    1066.0   test   \n",
       "20476                                           [122421]    1016.0   test   \n",
       "20477                                           [122425]    2080.0  train   \n",
       "20478                                           [122427]     986.0  train   \n",
       "20479                                           [122429]    1940.0  train   \n",
       "\n",
       "                filestem                                       wav_source  \n",
       "0           HH20220719-2       data/elicitation-wavs/wav/HH20220719-2.wav  \n",
       "1      HH20230414-Zoom-3  data/elicitation-wavs/wav/HH20230414-Zoom-3.wav  \n",
       "2           HH20220629-2       data/elicitation-wavs/wav/HH20220629-2.wav  \n",
       "3             HH20221127         data/elicitation-wavs/wav/HH20221127.wav  \n",
       "4       HH07242020-Zoom2   data/elicitation-wavs/wav/HH07242020-Zoom2.wav  \n",
       "...                  ...                                              ...  \n",
       "20475       HH08212020-2       data/elicitation-wavs/wav/HH08212020-2.wav  \n",
       "20476    HH20211207-Zoom    data/elicitation-wavs/wav/HH20211207-Zoom.wav  \n",
       "20477         HH20230516         data/elicitation-wavs/wav/HH20230516.wav  \n",
       "20478         HH20230626         data/elicitation-wavs/wav/HH20230626.wav  \n",
       "20479         HH20210707         data/elicitation-wavs/wav/HH20210707.wav  \n",
       "\n",
       "[20480 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del long_labels_df['wav_source']\n",
    "long_labels_df = long_labels_df.rename({'wav_source_mac': 'wav_source'}, axis=1)\n",
    "long_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long_labels_path = 'data/elicitation-wavs/autotranscribed/longlabels.csv'\n",
    "# long_labels_df.to_csv(long_labels_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20480.000000\n",
       "mean        97.360791\n",
       "std         89.925078\n",
       "min          3.000000\n",
       "25%         27.000000\n",
       "50%         55.000000\n",
       "75%        156.000000\n",
       "max        540.000000\n",
       "Name: transcription, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_labels_df = pd.read_csv(long_labels_path)\n",
    "long_labels_df['transcription'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_eval = long_labels_df['filestem']!='HH20210312'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asr_index', 'start', 'end', 'transcription', 'indices', 'duration',\n",
       "       'split', 'filestem', 'wav_source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\t67.947 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    20250.000000\n",
       "mean        12.079518\n",
       "std         10.931928\n",
       "min          0.240000\n",
       "25%          1.959250\n",
       "50%          7.393000\n",
       "75%         24.430250\n",
       "max         29.998000\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"total\\t{(long_labels_df.loc[not_eval,'duration']/3_600_000).sum():.3f} hours\")\n",
    "(long_labels_df.loc[not_eval,'duration']/1_000).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\t51.752 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    16384.000000\n",
       "mean        11.371341\n",
       "std         10.710856\n",
       "min          0.536000\n",
       "25%          1.950000\n",
       "50%          6.149500\n",
       "75%         23.052000\n",
       "max         29.998000\n",
       "Name: duration, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_df=asr_df.sort_values('asr_index')\n",
    "asr_df['filestem']=asr_df['eaf_path'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "long_labels_df=long_labels_df.sort_values('asr_index')\n",
    "\n",
    "long_labels_df.loc[asr_df['filestem']=='HH20210312', 'start']=asr_df.loc[asr_df['filestem']=='HH20210312', 'start']\n",
    "long_labels_df.loc[asr_df['filestem']=='HH20210312', 'end']=asr_df.loc[asr_df['filestem']=='HH20210312', 'end']\n",
    "long_labels_df['duration']=long_labels_df['end']-long_labels_df['start']\n",
    "\n",
    "is_train = long_labels_df['split']=='train'\n",
    "print(f\"total\\t{(long_labels_df.loc[is_train,'duration']/3_600_000).sum():.3f} hours\")\n",
    "(long_labels_df.loc[is_train,'duration']/1_000).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    16384.000000\n",
       "mean         3.291199\n",
       "std          1.244452\n",
       "min          1.000000\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          4.000000\n",
       "max         10.000000\n",
       "Name: transcription, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(asr_df.loc[asr_df['split']=='train', 'transcription'].str.split().apply(len).sum())\n",
    "asr_df.loc[asr_df['split']=='train', 'transcription'].str.split().apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "tira pct: 0.20980118393966649\n",
      "eng pct: 0.7374483955096096\n",
      "misc pct: 0.052750420550723875\n",
      "validation\n",
      "tira pct: 0.1478952520802741\n",
      "eng pct: 0.7918257464512971\n",
      "misc pct: 0.06027900146842878\n",
      "test\n",
      "tira pct: 0.07948278694295167\n",
      "eng pct: 0.8645014432215521\n",
      "misc pct: 0.05601576983549621\n"
     ]
    }
   ],
   "source": [
    "def add_words_by_lang(s: str, words_by_lang, word_counts_by_lang):\n",
    "    for w in s.split():\n",
    "        words_by_lang[get_word_language(w)].add(w)\n",
    "        word_counts_by_lang[get_word_language(w)]+=1\n",
    "word_counts_dict ={}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(split)\n",
    "    words_by_lang = {'tira': set(), 'eng': set(), 'misc': set()}\n",
    "    word_counts_by_lang = {'tira': 0, 'eng': 0, 'misc': 0}\n",
    "    long_labels_df.loc[long_labels_df['split']==split, 'transcription'].apply(lambda s: add_words_by_lang(s, words_by_lang, word_counts_by_lang))\n",
    "    word_counts_dict[split]=(word_counts_by_lang, words_by_lang)\n",
    "    total_words=sum(v for v in word_counts_by_lang.values())\n",
    "    print(f\"tira pct: {word_counts_by_lang['tira']/total_words}\")\n",
    "    print(f\"eng pct: {word_counts_by_lang['eng']/total_words}\")\n",
    "    print(f\"misc pct: {word_counts_by_lang['misc']/total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tira': 55749, 'eng': 195957, 'misc': 14017}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023654337392180896"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(55749-54923)/349196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tira': 65179, 'eng': 265150, 'misc': 18867}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Chumna.',\n",
       " 'Durotani',\n",
       " 'darja',\n",
       " 'coucou.',\n",
       " 'Lato,',\n",
       " 'Valais?',\n",
       " 'italian',\n",
       " 'Galando.',\n",
       " 'kukungapitito.',\n",
       " 'Kitcheno,',\n",
       " 'Malden.',\n",
       " 'Leker',\n",
       " 'Dhawanaksha',\n",
       " 'poto,',\n",
       " 'ngiyol',\n",
       " 'Ngaal.',\n",
       " 'karlen',\n",
       " 'case-mart.com.',\n",
       " 'Kukulu',\n",
       " 'vr',\n",
       " 'Shor.',\n",
       " 'kota.',\n",
       " 'Paule',\n",
       " \"N'kaabh.\",\n",
       " '98.',\n",
       " 'Hamid?',\n",
       " 'Otono.',\n",
       " 'low-low.',\n",
       " 'YAML.',\n",
       " 'ngui',\n",
       " 'NAPTO,',\n",
       " 'ezo.',\n",
       " 'Lali?',\n",
       " \"n'ele\",\n",
       " 'Lai',\n",
       " 'Dorna,',\n",
       " 'Loa',\n",
       " 'Kupu',\n",
       " 'P-O-N.',\n",
       " 'pri...',\n",
       " \"Kayla's\",\n",
       " 'ver',\n",
       " 'ngamu',\n",
       " 'kai',\n",
       " 'Intira.',\n",
       " 'Linga,',\n",
       " 'monto',\n",
       " \"d'ova\",\n",
       " 'Kamaishi,',\n",
       " 'Lai?',\n",
       " 'Arganna.',\n",
       " 'vala...',\n",
       " 'Karki',\n",
       " 'Runlock',\n",
       " 'kukunga,',\n",
       " 'not-that-many',\n",
       " 'au.',\n",
       " 'Kahino',\n",
       " 'buddhade.',\n",
       " 'loado.',\n",
       " \"po'luli,\",\n",
       " 'Kirlene,',\n",
       " 'K-O-N.',\n",
       " 'post-normal',\n",
       " 'ngau.',\n",
       " 'chee',\n",
       " 'toileta',\n",
       " 'ssawe.',\n",
       " 'nā',\n",
       " 'Odenina,',\n",
       " \"Wendy's.\",\n",
       " 'nyo,',\n",
       " 'Kakaar,',\n",
       " 'kāpā',\n",
       " 'vahara,',\n",
       " 'Ngave',\n",
       " 'Sleipnir.',\n",
       " 'ña,',\n",
       " 'ZQ.',\n",
       " 'una.',\n",
       " 'boli.',\n",
       " 'kukush',\n",
       " 'Kiyomo',\n",
       " 'chafdh',\n",
       " 'Laori',\n",
       " 'Ooley,',\n",
       " 'ndavo.',\n",
       " 'kongapote,',\n",
       " 'Kōkongo.',\n",
       " 'Lauda,',\n",
       " 'Cucu.',\n",
       " 'leechlu',\n",
       " 'AYIN',\n",
       " 'say-bys.',\n",
       " 'Yairi',\n",
       " 'Goto.',\n",
       " 'AMSI',\n",
       " 'Mina.',\n",
       " 'Kitchell?',\n",
       " 'igun.',\n",
       " 'lora',\n",
       " 'Dongno.',\n",
       " 'Santanis.',\n",
       " '115',\n",
       " 'Shalene.',\n",
       " 'Kukungu.',\n",
       " 'Lulib.',\n",
       " 'Diado.',\n",
       " 'krong',\n",
       " 'COVID-like.',\n",
       " 'Kukulkane.',\n",
       " 'grey',\n",
       " 'OLO.',\n",
       " 'ngamaan.',\n",
       " 'tēnā',\n",
       " 'kagari',\n",
       " 'Cumbre',\n",
       " 'Anundaba.',\n",
       " 'ira',\n",
       " 'adungna...',\n",
       " '39',\n",
       " 'shepard.',\n",
       " 'wengi,',\n",
       " \"l'bu.\",\n",
       " 'walele',\n",
       " 'Kukulkana',\n",
       " 'Kiddle',\n",
       " 'ngai.',\n",
       " 'De.',\n",
       " 'ngla',\n",
       " '112.',\n",
       " 'nekuku',\n",
       " \"they'll,\",\n",
       " 'lungi',\n",
       " 'Robo.',\n",
       " 'Naanu!',\n",
       " 'Nalai',\n",
       " 'Nani,',\n",
       " 'neo-ya.',\n",
       " 'ddra',\n",
       " 'nachi.',\n",
       " 'Ane.',\n",
       " 'Nge-nya.',\n",
       " 'Voto.',\n",
       " 'Oda,',\n",
       " '77',\n",
       " 'theatre,',\n",
       " 'Arthit',\n",
       " 'Do-do-do.',\n",
       " 'LUMO',\n",
       " 'อับเทUL',\n",
       " 'lir',\n",
       " 'Modori.',\n",
       " 'ngachile',\n",
       " 'dolar.',\n",
       " 'Romacho.',\n",
       " 'YOLO',\n",
       " 'Nyage',\n",
       " 'Kakra',\n",
       " 'Dotto',\n",
       " 'Ramda.',\n",
       " 'ringlo,',\n",
       " 'lume,',\n",
       " 'Treleni,',\n",
       " 'ari',\n",
       " 'Nirmala',\n",
       " 'kukumaga',\n",
       " 'Zuri',\n",
       " 'Lunduva.',\n",
       " 'dhúiní.',\n",
       " 'Gdoroto',\n",
       " 'Soringo',\n",
       " 'unera,',\n",
       " 'libredad',\n",
       " 'yegárato',\n",
       " 'koli.',\n",
       " 'Erdo',\n",
       " 'vrede,',\n",
       " 'Ngan.',\n",
       " \"Don't...\",\n",
       " \"They're.\",\n",
       " \"K'Chilot.\",\n",
       " 'Nyatida?',\n",
       " 'nyunga',\n",
       " 'Gotha.',\n",
       " 'Kukunga,',\n",
       " 'Erlene.',\n",
       " 'Doolidowda?',\n",
       " 'Liyadi',\n",
       " 'Nalabar.',\n",
       " 'Ning.',\n",
       " 'Reti.',\n",
       " 'pre-relevant.',\n",
       " 'vrdo,',\n",
       " 'Indova',\n",
       " 'Rar-Raw-Law,',\n",
       " 'ngambe,',\n",
       " 'kaku.',\n",
       " 'Teerlengi.',\n",
       " 'du,',\n",
       " 'kērā',\n",
       " 'alna',\n",
       " 'Urna.',\n",
       " \"m'boy\",\n",
       " 'Ndani.',\n",
       " \"Toto's.\",\n",
       " 'yicheletato,',\n",
       " '18a',\n",
       " 'la-may-dah',\n",
       " 'Yaki',\n",
       " '18...',\n",
       " '8F',\n",
       " 'Lamin',\n",
       " 'Svay',\n",
       " \"priya'minidh.\",\n",
       " 'dino',\n",
       " 'Kuli,',\n",
       " 'Arthol',\n",
       " 'TRI',\n",
       " 'Rwuli',\n",
       " 'Trau.',\n",
       " 'Hamin,',\n",
       " 'Pashto,',\n",
       " \"You'd\",\n",
       " 'ngvini',\n",
       " \"i'm\",\n",
       " 'K-E-P-E.',\n",
       " 'Moana.',\n",
       " 'down-stymied.',\n",
       " 'Kasha',\n",
       " 'alangare,',\n",
       " 'nyango?',\n",
       " 'ngayon,',\n",
       " 'leana.',\n",
       " 'Pupu',\n",
       " 'nguramana',\n",
       " 'ʻi',\n",
       " 'binano,',\n",
       " 'Gaeda',\n",
       " 'Yel.',\n",
       " 'Orlandito.',\n",
       " 'torreterleñi.',\n",
       " 'Ligi',\n",
       " 'libellary.',\n",
       " 'do-root.',\n",
       " 'pinot,',\n",
       " 'Irado,',\n",
       " 'Ngene.',\n",
       " 've',\n",
       " 'Tiro',\n",
       " \"p'yi,\",\n",
       " 'kakele.',\n",
       " 'Igun,',\n",
       " \"l'hope\",\n",
       " 'Agur.',\n",
       " 'Mm-hmm.',\n",
       " 'verlo,',\n",
       " 'Tasa,',\n",
       " 'Kinyo,',\n",
       " 'Kau',\n",
       " 'low-eating?',\n",
       " 'Nian',\n",
       " '38.',\n",
       " 'Kananga',\n",
       " 'Kichilano',\n",
       " 'ku.',\n",
       " 'spilt',\n",
       " \"le'ze.\",\n",
       " 'Ngai.',\n",
       " 'Dalanga,',\n",
       " 'Dagada?',\n",
       " 'Karcha',\n",
       " '2E.',\n",
       " 'berlin',\n",
       " 'ngin',\n",
       " 'downstep',\n",
       " 'lala.',\n",
       " \"That'd\",\n",
       " 'Yomo,',\n",
       " 'ngalbule',\n",
       " 'Piggott?',\n",
       " 'ngumbuksha',\n",
       " 'kubuli',\n",
       " 'L-I-D-I',\n",
       " 'Grr.',\n",
       " 'tivoli',\n",
       " 'dhalanga.',\n",
       " 'lada,',\n",
       " 'Nietzsútu.',\n",
       " \"n'ine'u\",\n",
       " 'Kukundi.',\n",
       " 'pre-yaduro',\n",
       " 'Chung-Hee',\n",
       " 'Kukuri.',\n",
       " \"there's.\",\n",
       " 'atomy.',\n",
       " '9',\n",
       " 'Sivitz.',\n",
       " 'Ulunggu',\n",
       " 'L-A-D-U-R-I.',\n",
       " 'Yashio',\n",
       " 'OLO',\n",
       " 'Siri',\n",
       " 'Lachy.',\n",
       " 'Dharia.',\n",
       " 'loi,',\n",
       " 'kakang?',\n",
       " 'Yate',\n",
       " 'Ngaramu,',\n",
       " \"n'eba?\",\n",
       " 'alathwa',\n",
       " '35.',\n",
       " 'Jai',\n",
       " 'Cantholi.',\n",
       " 'n-g-e...',\n",
       " 'Káraoge,',\n",
       " 'kāngi',\n",
       " 'duro?',\n",
       " 'pêche?',\n",
       " 'Sarlan',\n",
       " 'vangad',\n",
       " 'Gautina.',\n",
       " 'yengche',\n",
       " 'kookukai,',\n",
       " 'Tula.',\n",
       " 'Bam,',\n",
       " 'edel',\n",
       " 'Lourneau.',\n",
       " 'Kalamata,',\n",
       " 'Lenggen.',\n",
       " 'R-L-L-L-O.',\n",
       " '321.8.',\n",
       " 'mojo',\n",
       " 'Aljigah...',\n",
       " 'lapla,',\n",
       " \"N'gine.\",\n",
       " 'Kukum',\n",
       " 'kāne.',\n",
       " '68',\n",
       " 'väl',\n",
       " 'NGEERA.',\n",
       " \"Ng'ung'u\",\n",
       " 'IDO.',\n",
       " 'orna',\n",
       " 'dda',\n",
       " 'sif,',\n",
       " \"K'ch'luw.\",\n",
       " \"a'pwyni\",\n",
       " 'Elnya',\n",
       " 'C-E-R-C-E.',\n",
       " 'Oragra,',\n",
       " 'Arangator,',\n",
       " 'angmar',\n",
       " 'langa',\n",
       " \"I've,\",\n",
       " 'kukungapamama.',\n",
       " 'I-S-R-O.',\n",
       " 'Ota.',\n",
       " 'age-wise.',\n",
       " 'Rto',\n",
       " 'pre-yatvalomu.',\n",
       " 'cetera.',\n",
       " 'ancho,',\n",
       " 'Garlandio.',\n",
       " 'Lume,',\n",
       " 'Ngana',\n",
       " 'appen-thesis.',\n",
       " 'Labo',\n",
       " 'kaningi.',\n",
       " 'Mono-aure?',\n",
       " 'Anya.',\n",
       " 'ngīngi',\n",
       " 'Aedon.',\n",
       " 'pulvergy.',\n",
       " \"un'ora,\",\n",
       " 'Oo.',\n",
       " 'ngan,',\n",
       " 'lemonen,',\n",
       " 'EO?',\n",
       " 'Kichung,',\n",
       " 'dhe',\n",
       " 'OSI',\n",
       " 'kabi,',\n",
       " 'Hamada?',\n",
       " 'ratorun',\n",
       " 'Shwetanis.',\n",
       " 'sharon',\n",
       " 'N-U-M-I.',\n",
       " 'hymena.',\n",
       " 'ngi.',\n",
       " 'slidy?',\n",
       " 'Láin',\n",
       " 'Idlel',\n",
       " 'ke',\n",
       " 'Tolnya.',\n",
       " 'Namtera,',\n",
       " 'ngung',\n",
       " 'abo',\n",
       " 'Gaita',\n",
       " 'Kolita,',\n",
       " 'kukukua',\n",
       " 'Mulu,',\n",
       " 'yalala.',\n",
       " 'viyare',\n",
       " 'Kava,',\n",
       " \"a'a'.\",\n",
       " 'choychi',\n",
       " 'Asa,',\n",
       " 'Jata',\n",
       " 'lal,',\n",
       " 'Nalingwa,',\n",
       " 'GC?',\n",
       " 'Laar.',\n",
       " 'karoge',\n",
       " 'laj',\n",
       " 'Schwoz,',\n",
       " 'ngamlare',\n",
       " 'lau',\n",
       " 'Kamerla',\n",
       " 'Chita?',\n",
       " \"k'nona\",\n",
       " 'Lengok.',\n",
       " 'artol',\n",
       " 'Guite',\n",
       " 'Valanai.',\n",
       " 'garra',\n",
       " 'anmola',\n",
       " \"Nyung'o.\",\n",
       " 'kukui.',\n",
       " 'Yaduro',\n",
       " 'Backcase.',\n",
       " 'tagwe',\n",
       " 'Hingo.',\n",
       " 'kaa.',\n",
       " 'Mookoo',\n",
       " 'surrito?',\n",
       " 'carlinghi',\n",
       " 'al-al',\n",
       " 'heart-falling.',\n",
       " 'verdana.',\n",
       " 'Dio',\n",
       " 'Amidam.',\n",
       " 'Erad',\n",
       " 'zho,',\n",
       " 'kena',\n",
       " 'Saisa.',\n",
       " 'Inga,',\n",
       " 'Thay',\n",
       " 'nara',\n",
       " 'Verceache.',\n",
       " 'LaDory.',\n",
       " \"N'Churru.\",\n",
       " 'well-designed',\n",
       " 'Yela',\n",
       " 'kukungu',\n",
       " 'chalup.',\n",
       " 'Kandoli.',\n",
       " 'kukulu,',\n",
       " \"It'll\",\n",
       " 'Nalungon.',\n",
       " 'four-way',\n",
       " 'Caradoly.',\n",
       " 'Dulboch,',\n",
       " '18,',\n",
       " 'LQ',\n",
       " 'dewelea.',\n",
       " 'loro.',\n",
       " 'Lundy,',\n",
       " 'Tula,',\n",
       " 'ngekuku?',\n",
       " 'Apnea.',\n",
       " 'Nipa',\n",
       " 'Nichilo.',\n",
       " 'Ndova,',\n",
       " 'curi.',\n",
       " 'Koongaachee!',\n",
       " 'raqqitino',\n",
       " 'V-O-Y-A.',\n",
       " 'LV.',\n",
       " 'Nol.',\n",
       " 'kiyomaman',\n",
       " 'Ley,',\n",
       " 'Ojo,',\n",
       " 'Yush,',\n",
       " 'Brondby.',\n",
       " 'Vivala,',\n",
       " 'double-sided',\n",
       " \"L'atvar.\",\n",
       " 'kukun.',\n",
       " 'Orchacha,',\n",
       " 'kakar',\n",
       " 'Laaar.',\n",
       " 'Tierra',\n",
       " 'Maclennan.',\n",
       " 'lenguendo.',\n",
       " 'un,',\n",
       " 'Aprena',\n",
       " 'ratorun.',\n",
       " 'Nanocha.',\n",
       " \"N'yol.\",\n",
       " 'Ngela,',\n",
       " 'adungna',\n",
       " 'Nano',\n",
       " 'Lalo.',\n",
       " '15.',\n",
       " 'kakotʻo',\n",
       " 'boop.',\n",
       " 'ano',\n",
       " 'ngTree.',\n",
       " '3A.',\n",
       " 'Avan',\n",
       " \"l'anji.\",\n",
       " 'Kaanga.',\n",
       " 'mm-hmm.',\n",
       " 'Kinyarra.',\n",
       " 'Toini',\n",
       " 'Yidam.',\n",
       " 'honore.',\n",
       " 'riabro.',\n",
       " 'EI.',\n",
       " 'Needly.',\n",
       " 'bool.',\n",
       " 'Ndobage?',\n",
       " 'iraq',\n",
       " 'Nnele.',\n",
       " 'Larchu',\n",
       " 'condinom.',\n",
       " 'Blibby',\n",
       " \"ong'o\",\n",
       " \"anybody's\",\n",
       " 'teñe-la,',\n",
       " 'yeet.',\n",
       " 'Konya.',\n",
       " 'nya.',\n",
       " 'Ojebete',\n",
       " 'Chitra',\n",
       " 'Wai.',\n",
       " 'Heh.',\n",
       " 'uncheathe.',\n",
       " \"lo'a\",\n",
       " \"S'abodo.\",\n",
       " 'queso.',\n",
       " 'Rashaam',\n",
       " 'Krichardt,',\n",
       " 'Kumbhura',\n",
       " 'yellow-lined',\n",
       " 'Kukumura,',\n",
       " 'unde',\n",
       " 'Lorato,',\n",
       " 'kiyomomo',\n",
       " 'lumbe.',\n",
       " 'Gwana.',\n",
       " 'Lingen.',\n",
       " 'Lemayang.',\n",
       " 'Toa.',\n",
       " 'Unere',\n",
       " 'Aram',\n",
       " 'kuku.github.com.',\n",
       " 'voluto',\n",
       " 'dalavavra,',\n",
       " 'R-Way.',\n",
       " 'Kalano',\n",
       " 'unare.',\n",
       " \"N'gello.\",\n",
       " 'wew',\n",
       " 'Andover.',\n",
       " 'Condi',\n",
       " 'Aang',\n",
       " 'Lutva',\n",
       " 'boobie,',\n",
       " 'changrel',\n",
       " 'ñala,',\n",
       " 'L-U-R-A.',\n",
       " 'Sertarlan,',\n",
       " 'Cartenit,',\n",
       " 'rrr',\n",
       " 'than...Nah!',\n",
       " \"didn't.\",\n",
       " 'donna',\n",
       " 'Chivito',\n",
       " 'Opry.',\n",
       " 'Mida?',\n",
       " 'Nichilo,',\n",
       " 'Kurji.',\n",
       " 'tamanta.',\n",
       " 'Elara.',\n",
       " \"you're...\",\n",
       " 'undecumbeneed?',\n",
       " \"Gord's\",\n",
       " 'async',\n",
       " 'Navi.',\n",
       " 'kiyo',\n",
       " 'アナ,',\n",
       " '20?',\n",
       " 'yali.',\n",
       " 'dengla,',\n",
       " 'ngatata',\n",
       " 'Ke',\n",
       " 'homophonous.',\n",
       " 'Kamonto.',\n",
       " 'how...durña,',\n",
       " 'arañgare?',\n",
       " \"n'Oruv'a\",\n",
       " 'reyhwun.',\n",
       " 'NGU.',\n",
       " 'M-U-T-T-I-N-O.',\n",
       " 'ngga',\n",
       " 'fairytale,',\n",
       " 'LaRue.',\n",
       " \"ng'plu\",\n",
       " 'Ongo',\n",
       " 'joll',\n",
       " 'UNERE',\n",
       " 'minyak,',\n",
       " 'LOMOZ.',\n",
       " 'Veku',\n",
       " 'Exaggerative.',\n",
       " 'CSR.',\n",
       " '19.',\n",
       " 'Lorno,',\n",
       " 'GQ.',\n",
       " \"N'Bu.\",\n",
       " \"kukumuledin'ya.\",\n",
       " 'chickbooks,',\n",
       " 'H0',\n",
       " 'Nebo.',\n",
       " 'ngakal',\n",
       " 'vere,',\n",
       " 'Lululu,',\n",
       " 'kermna.',\n",
       " 'Nasa.',\n",
       " 'Chilu,',\n",
       " 'Larrow.',\n",
       " 'lupa',\n",
       " 'g-note',\n",
       " 'Chechi',\n",
       " 'Kawele.',\n",
       " 'Mora.',\n",
       " 'Kaka',\n",
       " 'Yotomo.',\n",
       " 'looty',\n",
       " 'nal,',\n",
       " 'gigi',\n",
       " \"T'uani.\",\n",
       " 'yeh',\n",
       " '76.',\n",
       " 'Khor',\n",
       " \"they'll\",\n",
       " 'Elo',\n",
       " 'aprena,',\n",
       " \"g'u.\",\n",
       " 'grandmum',\n",
       " 'de.',\n",
       " 'Roma,',\n",
       " 'Zold.',\n",
       " 'ngapo',\n",
       " 'kukung',\n",
       " 'Gato.',\n",
       " 'KELOLAND',\n",
       " 'levra',\n",
       " 'drummy,',\n",
       " 'nda?',\n",
       " 'tārehāte,',\n",
       " 'Kwaanaana.',\n",
       " 'Nale',\n",
       " 'Dandangacina',\n",
       " 'Lomohelo.',\n",
       " 'sparrow-like?',\n",
       " 'leadar,',\n",
       " 'Kirlenji,',\n",
       " 'ngatana...',\n",
       " 'S-R-U-R-O.',\n",
       " 'urnota',\n",
       " 'Unnara.',\n",
       " 'nyala',\n",
       " 'ngin...',\n",
       " 'Iyal',\n",
       " 'Amale',\n",
       " 'Kwe',\n",
       " 'Ingo',\n",
       " 'NDOBA?',\n",
       " 'Chacha.',\n",
       " \"I'm-\",\n",
       " 'ta-to-su-ru.',\n",
       " 'Irado',\n",
       " 'varro.',\n",
       " 'arbol',\n",
       " 'Ngadia.',\n",
       " 'Lodo',\n",
       " '90...',\n",
       " 'dhalam.',\n",
       " 'Lenga',\n",
       " 'Al-Bidash',\n",
       " 'cotto.',\n",
       " 'jammer.',\n",
       " 'Deol',\n",
       " \"L'Antho.\",\n",
       " 'Kori-ba.',\n",
       " 'hummick?',\n",
       " 'ʻāke',\n",
       " 'Apre.',\n",
       " 'R1.',\n",
       " 'Ñalabada,',\n",
       " 'atavedar?',\n",
       " 'muduri...',\n",
       " 'Lechon,',\n",
       " 'GDG,',\n",
       " 'vavra',\n",
       " 'kvledana',\n",
       " 'náre',\n",
       " 'Tarote.',\n",
       " 'nieleciak',\n",
       " 'Tira',\n",
       " 'ngune',\n",
       " 'Rotundi.',\n",
       " 'TIRA.',\n",
       " 'Nya.',\n",
       " 'Garoto.',\n",
       " '121',\n",
       " 'Wulik',\n",
       " 'WS,',\n",
       " 'Nerege',\n",
       " 'secretism',\n",
       " 'Jimena?',\n",
       " '56',\n",
       " 'nong',\n",
       " 'yun',\n",
       " 'uh...me',\n",
       " 'iya,',\n",
       " 'FG.',\n",
       " 'ngI.',\n",
       " 'kwan',\n",
       " 'ʻāre.',\n",
       " 'Larole',\n",
       " \"Na'ngi'ye'.\",\n",
       " 'lingi,',\n",
       " 'Mmm.',\n",
       " 'WR',\n",
       " 'AWWUR,',\n",
       " \"y'all.\",\n",
       " 'K-U-M-O.',\n",
       " 'tuku',\n",
       " 'un.',\n",
       " 'URMUZO.',\n",
       " 'nd?',\n",
       " 'rle',\n",
       " 'chachivito.',\n",
       " 'riu',\n",
       " 'ko.',\n",
       " 'ndoba,',\n",
       " 'kagaro',\n",
       " \"Nyang'o.\",\n",
       " 'kukukungai',\n",
       " 'chela',\n",
       " 'Goofoo',\n",
       " 'Calcio',\n",
       " 'Igarra',\n",
       " '28',\n",
       " 'Akshay.',\n",
       " 'lewa',\n",
       " 'दङगल.',\n",
       " 'HLO',\n",
       " 'Olchechizito.',\n",
       " 'Kallair',\n",
       " 'kurita,',\n",
       " 'abudo,',\n",
       " 'Terlaney?',\n",
       " 'lola.',\n",
       " 'TIR-le-NYE.',\n",
       " 'Nyena.',\n",
       " 'VKIDO...',\n",
       " 'negedol',\n",
       " 'Tanana.',\n",
       " '34,',\n",
       " 'vertice.',\n",
       " \"n'hédo\",\n",
       " 'kare',\n",
       " 'Sertal.',\n",
       " 'I-L-E-R-A-N.',\n",
       " 'prix.',\n",
       " 'L-A-L-U,',\n",
       " 'Uri.',\n",
       " 'Ngolingo',\n",
       " 'Anu',\n",
       " 'Vivre',\n",
       " 'EAP.',\n",
       " 'I-S-A-N-G-E-N-E.',\n",
       " '1010.',\n",
       " 'Kukulia.',\n",
       " 'Larmna...',\n",
       " '102',\n",
       " 'Buh.',\n",
       " 'ngadhini',\n",
       " 'Ertl,',\n",
       " \"N'gala\",\n",
       " 'ngu',\n",
       " 'Rejo.',\n",
       " 'Nyango',\n",
       " 'Ata.',\n",
       " '2.',\n",
       " 'Caddo',\n",
       " 'lengai...',\n",
       " 'Elbrali',\n",
       " 'drotha?',\n",
       " 'kanto?',\n",
       " 'Khurji.',\n",
       " 'Tavey.',\n",
       " 'Oneira.',\n",
       " 'Indoba?',\n",
       " \"another's\",\n",
       " 'Yedoledo',\n",
       " 'Ganga',\n",
       " 'Nakalungu.',\n",
       " 'UNRSS.',\n",
       " 'Rrraa,',\n",
       " 'nyungi?',\n",
       " 'Grav',\n",
       " \"who's.\",\n",
       " 'N-A-M-L-Z.',\n",
       " 'Machuach',\n",
       " 'yarje',\n",
       " 'Talan',\n",
       " 'Kukwu',\n",
       " 'si...',\n",
       " 'KK',\n",
       " 'Mují,',\n",
       " 'IDI.',\n",
       " 'vers',\n",
       " 'Zaya.',\n",
       " 'Nantol.',\n",
       " 'she-bot.',\n",
       " 'Kapitito',\n",
       " 'Tol',\n",
       " 'Covarrlea',\n",
       " '114',\n",
       " 'Lurie',\n",
       " 'Double-gay.',\n",
       " 'Vai!',\n",
       " 'mamurin.',\n",
       " \"ch'olo.\",\n",
       " \"Bethany's\",\n",
       " 'Kameachi,',\n",
       " 'rungu.',\n",
       " \"ng'ore.\",\n",
       " 'ASA',\n",
       " 'dia,',\n",
       " 'verleda,',\n",
       " \"should've...\",\n",
       " 'Kangana.',\n",
       " 'Chingi',\n",
       " 'Kirlian.',\n",
       " \"d'Intex.\",\n",
       " 'Lolia',\n",
       " '12',\n",
       " '7th',\n",
       " 'iyun',\n",
       " 'lallir?',\n",
       " 'imperfectives',\n",
       " 'nere.',\n",
       " 'Kachee!',\n",
       " 'Jalito,',\n",
       " 'honnêt.',\n",
       " '73',\n",
       " 'Yonamu,',\n",
       " 'Nool',\n",
       " 'Mackey',\n",
       " 'Kagito.',\n",
       " 'roba',\n",
       " 'artinot.',\n",
       " 'Karkov',\n",
       " \"doesn't\",\n",
       " 'naro.',\n",
       " 'preduye,',\n",
       " 'ngacholo,',\n",
       " 'de,',\n",
       " 'Kapitaka',\n",
       " 'Tau-ini',\n",
       " 'Kani.',\n",
       " 'Urnawkaro',\n",
       " 'SNL',\n",
       " 'Two-line',\n",
       " 'LD.',\n",
       " 'akia',\n",
       " 'la...she...',\n",
       " 'ZOMOCO,',\n",
       " 'Pranazan',\n",
       " 'Kosewi.',\n",
       " 'libele',\n",
       " 'kukum.',\n",
       " 'jords',\n",
       " 'Mucho',\n",
       " 'Uji,',\n",
       " 'Vom.',\n",
       " 'mlare.',\n",
       " 'Bolo?',\n",
       " 'NGE.',\n",
       " 'Tahu.',\n",
       " 'Kiffs.',\n",
       " 'erina.',\n",
       " 'mora.',\n",
       " 'piti',\n",
       " 'aga',\n",
       " 'Lando.',\n",
       " 'leh',\n",
       " 'Evo',\n",
       " 'Alo,',\n",
       " 'Orla',\n",
       " 'Nurtle.',\n",
       " 'Lingi,',\n",
       " 'Chingorani',\n",
       " 'colours.',\n",
       " 'le-ray',\n",
       " 'Tirac,',\n",
       " 'Jitri.',\n",
       " 'Kedro,',\n",
       " 'Veribee?',\n",
       " 'LVEEE,',\n",
       " 'nguduri.',\n",
       " 'Ritchie,',\n",
       " 'Karna',\n",
       " 'teño',\n",
       " 'Kaurna,',\n",
       " 'chido',\n",
       " 'Kinga,',\n",
       " 'T-Rot',\n",
       " 'Ja.',\n",
       " \"k'inngi\",\n",
       " 'Rigi,',\n",
       " 'ادلال',\n",
       " 'widney-o',\n",
       " 'ngat',\n",
       " 'Mojo.',\n",
       " 'pinna.',\n",
       " 'LUMO.',\n",
       " 'Larley',\n",
       " 'chedadan.',\n",
       " 'Dagara.',\n",
       " 'Hehe.',\n",
       " 'Langen.',\n",
       " 'yato,',\n",
       " 'dangal',\n",
       " 'Kato,',\n",
       " 'kāpi',\n",
       " \"Y'all\",\n",
       " 'ALS.',\n",
       " 'toni,',\n",
       " 'NNN',\n",
       " 'cordiaceae?',\n",
       " 'deminant',\n",
       " 'Kangi',\n",
       " 'laro',\n",
       " 'Oblu',\n",
       " 'valorizing',\n",
       " 'Nyarla',\n",
       " 'Kuku,',\n",
       " 'Janiel',\n",
       " \"N'goba.\",\n",
       " 'Yalal',\n",
       " 'koutou',\n",
       " 'ng-ma',\n",
       " 'ella,',\n",
       " 'Nidji',\n",
       " 'Gat-d-v.',\n",
       " 'Kallera',\n",
       " 'Kabuli-anga.',\n",
       " \"wha'eva\",\n",
       " 'K-A-R-C-A.',\n",
       " 'Parsvan.',\n",
       " 'Niren...',\n",
       " \"T'ai\",\n",
       " 'hwoa',\n",
       " 'vglchah.',\n",
       " \"n'eba.\",\n",
       " \"I'm-a-yo.\",\n",
       " 'ade.',\n",
       " \"there's\",\n",
       " 'ngemlare.',\n",
       " 'Unere,',\n",
       " 'Yan',\n",
       " 'Erlange.',\n",
       " 'Kunere',\n",
       " 'Hola.',\n",
       " 'Chikwu',\n",
       " 'Eda',\n",
       " 'Aveniene,',\n",
       " 'Lejela.',\n",
       " 'Loko',\n",
       " \"Caballé-Dol-L'Ongo?\",\n",
       " 'Asa',\n",
       " 'Cangy',\n",
       " 'bengal',\n",
       " 'ngati,',\n",
       " 'Lomote.',\n",
       " 'Ojot.',\n",
       " 'rnona.',\n",
       " 'Leverle,',\n",
       " 'M-U-R.',\n",
       " 'Hime',\n",
       " 'UDE.',\n",
       " 'regalar',\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_counts_by_lang)\n",
    "words_by_lang['misc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
