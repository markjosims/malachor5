{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf4a9e9",
   "metadata": {},
   "source": [
    "# tira_asr_noise\n",
    "Measure correlation between various nosie measures on Tira ASR dataset\n",
    "Sample 5 rows from each 10 bins of each measure and set aside for hand eval,\n",
    "then visualize correlation of hand eval with objective measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259410e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\malachor5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e9966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = r'C:\\projects\\malachor5\\data\\dataset_clips\\tira-asr\\unprocessed_audio_ds'\n",
    "hand_eval_dir = r\"C:\\projects\\malachor5\\data\\dataset_clips\\tira-asr\\hand-eval\"\n",
    "os.makedirs(hand_eval_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c83574c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'clip_name', 'index', 'vad_chunks', 'speech_embed', 'text_embed', 'embed_cos_sim', 'wada_snr', 'nist_snr', 'duration', 'vad_duration', 'vad_pct'],\n",
       "    num_rows: 23261\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_disk(ds_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd1a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vad_duration vad_pct PearsonRResult(statistic=np.float64(-0.058665296830645614), pvalue=np.float64(3.409606635791288e-19))\n",
      "vad_duration embed_cos_sim PearsonRResult(statistic=np.float64(-0.047300970979111956), pvalue=np.float64(5.284043368062316e-13))\n",
      "vad_duration wada_snr PearsonRResult(statistic=np.float64(0.08268366055430638), pvalue=np.float64(1.4155284764961584e-36))\n",
      "vad_duration nist_snr PearsonRResult(statistic=np.float64(0.14086636331862154), pvalue=np.float64(2.1873407475885328e-103))\n",
      "vad_pct embed_cos_sim PearsonRResult(statistic=np.float64(0.18004735228096272), pvalue=np.float64(1.0561006451866381e-168))\n",
      "vad_pct wada_snr PearsonRResult(statistic=np.float64(-0.3433066376292787), pvalue=np.float64(0.0))\n",
      "vad_pct nist_snr PearsonRResult(statistic=np.float64(-0.29215886904184274), pvalue=np.float64(0.0))\n",
      "embed_cos_sim wada_snr PearsonRResult(statistic=np.float64(-0.028318656206717613), pvalue=np.float64(1.5626254087926876e-05))\n",
      "embed_cos_sim nist_snr PearsonRResult(statistic=np.float64(-0.008147257914243944), pvalue=np.float64(0.21403829393412174))\n",
      "wada_snr nist_snr PearsonRResult(statistic=np.float64(0.7318030036489218), pvalue=np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "quality_measures = ['vad_duration', 'vad_pct', 'embed_cos_sim', 'wada_snr', 'nist_snr']\n",
    "for i, measure_1 in enumerate(quality_measures[:-1]):\n",
    "    for j, measure_2 in enumerate(quality_measures[i+1:]):\n",
    "        corr = pearsonr(ds[measure_1], ds[measure_2])\n",
    "        print(measure_1, measure_2, corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccce9760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vad_duration\n",
      "vad_pct\n",
      "embed_cos_sim\n",
      "wada_snr\n",
      "nist_snr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.int64(5039),\n",
       "  np.int64(15529),\n",
       "  np.int64(17738),\n",
       "  np.int64(6114),\n",
       "  np.int64(2741)],\n",
       " 250)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_added = []\n",
    "bin_sample_n = 5\n",
    "for measure in quality_measures:\n",
    "    print(measure)\n",
    "    bins = mquantiles(ds[measure], prob=np.linspace(0,1,10))\n",
    "    binned_measure = np.digitize(ds[measure], bins)\n",
    "    for bin in range(1, bins.shape[0]+1):\n",
    "        # print(bin, binned_measure[binned_measure==bin].shape)\n",
    "        bin_idcs = np.argwhere(binned_measure==bin).squeeze()\n",
    "        for _ in range(bin_sample_n):\n",
    "            new_idx = np.random.choice(bin_idcs)\n",
    "            while new_idx in rows_added:\n",
    "                new_idx = np.random.choice(bin_idcs)\n",
    "            rows_added.append(new_idx)\n",
    "rows_added[:5], len(rows_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a72f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'clip_name', 'index', 'vad_chunks', 'speech_embed', 'text_embed', 'embed_cos_sim', 'wada_snr', 'nist_snr', 'duration', 'vad_duration', 'vad_pct'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_eval_ds = ds.select(rows_added)\n",
    "hand_eval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1857f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 250/250 [00:03<00:00, 65.70 examples/s] \n"
     ]
    }
   ],
   "source": [
    "wav_files = []\n",
    "def row_to_wav(row, outdir: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads samples from a Dataset Audio row (in the form of a dictionary)\n",
    "    and save as a wav in the specified `outdir`.\n",
    "    \"\"\"\n",
    "    basepath = os.path.basename(row['clip_name'])\n",
    "    path = os.path.join(outdir, basepath)\n",
    "    wav_files.append(path)\n",
    "    wav_array = row['audio']['array']\n",
    "    sr = row['audio']['sampling_rate']\n",
    "    # cast to torch tensor and make 2D\n",
    "    wav_tensor = torch.Tensor(wav_array).unsqueeze(0)\n",
    "    torchaudio.save(path, wav_tensor, sr)\n",
    "    return {\"filename\": os.path.basename(path)}\n",
    "\n",
    "df = hand_eval_ds.map(lambda row: row_to_wav(row, hand_eval_dir), remove_columns=hand_eval_ds.column_names).to_pandas()\n",
    "likert_cols = [\n",
    "    \"Audio quality\",\n",
    "    \"Crosstalk\",\n",
    "    \"Only Tira spoken?\",\n",
    "    \"Disfluencies?\",\n",
    "    \"Tira transcription accuracy\",\n",
    "]\n",
    "for col in likert_cols:\n",
    "    df[col]=''\n",
    "df.to_excel(os.path.join(hand_eval_dir, 'eval.xlsx'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
