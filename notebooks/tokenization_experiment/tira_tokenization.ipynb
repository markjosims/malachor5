{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Make token vocabulary\n",
    "Train a tokenizer on the Tira ASR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import WhisperTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata='/Users/markjos/projects/malachor5/data/tira-clean/metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = \"[UNK]\"\n",
    "trainer=trainers.WordPieceTrainer(vocab_size=800, special_tokens=[unk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(metadata)\n",
    "def get_text_corpus():\n",
    "    for row in df['transcription']:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=unk))\n",
    "tokenizer.train_from_iterator(get_text_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('là t̪ə́rvàvɛ̀',\n",
       " ['là ', '##t̪', '##ə́r', '##v', '##àv', '##ɛ̀'],\n",
       " [477, 94, 479, 49, 312, 87])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line=df['transcription'][5000]\n",
    "encoding=tokenizer.encode(line)\n",
    "\n",
    "line, encoding.tokens, encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.44875"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tira_vocab=tokenizer.get_vocab()\n",
    "vocab_lens=[]\n",
    "for item in tira_vocab:\n",
    "    vocab_lens.append(len(item.strip().replace('#', '')))\n",
    "np.array(vocab_lens).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tira_id2tok={v:k for k, v in tira_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                         [lə̀və̀lɛ̀ð, ##ɛ́l , ##únɛ̀ɾɛ̀]\n",
       " 1                            [kə̀, ##ŋà, ##c, ##î, ##í]\n",
       " 2              [ŋ̀g, ##átɛ́, ##və́lɛ̂ðɔ́ nd̪ɔ̀bà, ##gɛ̀]\n",
       " 3        [ŋ, ##ɔ́ð, ##ɔ́ ŋ, ##á , ##və́lɛ̀ðà ð, ##àŋ...\n",
       " 4                          [ðə̀, ##və̀l, ##è, ##ðɔ̀, ##ŋ]\n",
       "                                ...                        \n",
       " 20475    [ðá , ##nɛ́, ##l, ##ê, ## , ##və̀lɛ̀ðɔ́ nd̪ɔ...\n",
       " 20476                                    [ŋ, ##ə̀v, ##rà]\n",
       " 20477       [ŋ, ##ə̀, ##bú, ##r, ##ŋ, ##ɛ̀ , ##án, ##ó]\n",
       " 20478    [lá , ##vŕ, ##ð, ##ìt̪, ##ɔ̀ , ##ku, ##ku, ...\n",
       " 20479    [ùrnɔ̀ , ##kə̀, ##ŋ, ##a, ##cí , ##ápr, ##i...\n",
       " Name: transcription, Length: 20480, dtype: object,\n",
       " 0                                          [394, 517, 280]\n",
       " 1                                 [359, 211, 73, 255, 100]\n",
       " 2                                     [526, 569, 675, 199]\n",
       " 3                  [25, 697, 149, 111, 518, 157, 564, 108]\n",
       " 4                                 [561, 136, 139, 263, 57]\n",
       "                                ...                        \n",
       " 20475                         [362, 629, 45, 508, 46, 627]\n",
       " 20476                                       [25, 319, 696]\n",
       " 20477                 [25, 91, 509, 72, 57, 121, 114, 159]\n",
       " 20478    [273, 646, 53, 541, 101, 786, 786, 57, 54, 46,...\n",
       " 20479               [478, 406, 57, 43, 204, 494, 131, 226]\n",
       " Name: transcription, Length: 20480, dtype: object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tira_ids=df['transcription'].apply(lambda s: tokenizer.encode(s).ids)\n",
    "tira_tokenized=df['transcription'].apply(lambda s: tokenizer.encode(s).tokens)\n",
    "tira_tokenized, tira_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142409,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_concat=np.concatenate(tira_tokenized)\n",
    "tokenized_corpus_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "##ùð            100\n",
       "##ì ùnɛ̀ɾɛ̀    100\n",
       "##wà            100\n",
       "##cí ŋ          100\n",
       "##rí            100\n",
       "                ... \n",
       "##ùŋù            1\n",
       "lùrnɔ̀            1\n",
       "##ɔɽ               1\n",
       "##ɔ̀nà            1\n",
       "ɛ                  1\n",
       "Name: count, Length: 360, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts=pd.Series(tokenized_corpus_concat).value_counts()\n",
    "token_counts[token_counts<=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(tokenized_corpus_concat).unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predicted Whisper tokens\n",
    "Run Whisper tokenizer on transcribed Tira ASR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>croatian</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HH01082021-m03s37ms011-m03s39ms552.wav</td>\n",
       "      <td>A prije jedi za hala.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HH01082021-m03s41ms371-m03s45ms023.wav</td>\n",
       "      <td>Apri-jadi-vanhala.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HH01082021-m04s43ms401-m04s46ms528.wav</td>\n",
       "      <td>A prije je vledo da hvala.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HH01082021-m04s48ms835-m04s51ms580.wav</td>\n",
       "      <td>A prije je vledo za hala.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HH01082021-m05s04ms737-m05s07ms762.wav</td>\n",
       "      <td>Apre je vledo za mala.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     path                     croatian  split\n",
       "0  HH01082021-m03s37ms011-m03s39ms552.wav        A prije jedi za hala.  train\n",
       "1  HH01082021-m03s41ms371-m03s45ms023.wav           Apri-jadi-vanhala.  train\n",
       "2  HH01082021-m04s43ms401-m04s46ms528.wav   A prije je vledo da hvala.  train\n",
       "3  HH01082021-m04s48ms835-m04s51ms580.wav    A prije je vledo za hala.  train\n",
       "4  HH01082021-m05s04ms737-m05s07ms762.wav       Apre je vledo za mala.  train"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_labels = '/Users/markjos/projects/malachor5/data/tira-asr/tira-clean-split-transcribed.csv'\n",
    "\n",
    "whisper_df=pd.read_csv(whisper_labels)\n",
    "whisper_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's merge the Whisper and Tira metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20480, 29), (20480, 3))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, whisper_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20480, 31)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge=pd.merge(df, whisper_df, on='path')\n",
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge.to_csv(metadata, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['start', 'end', 'transcription', 'eaf_source', 'wav_source',\n",
       "       'raw_transcription', 'clip', 'wav_rawpath', 'file_name', 'path',\n",
       "       'allosaurus', 'clap_ipa_cos_sim', 'wada_snr', 'nist_stnr',\n",
       "       'speaker-diarization-3.1', 'voice-activity-detection',\n",
       "       'whisper-large-v3', 'clapipa-transcription-allosaurus', 'vad_s',\n",
       "       'drz_s', 'trans_len', 'pcnt_speech', 'trans_len_sq', 'trans_len_log',\n",
       "       'cos_sim_softmax', 'cos_sim_log', 'duration', 'split', 'croatian'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split']=df['split_x']\n",
    "df['croatian']=df['croatian_x']\n",
    "df=df.drop(['split_x', 'split_y', 'croatian_x', 'croatian_y'], axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get Whisper tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_tok=WhisperTokenizer.from_pretrained('openai/whisper-large-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [287, 73, 836, 706, 13]\n",
       "1                           [350, 6, 77, 6, 64, 22392, 72]\n",
       "2        [220, 273, 6908, 1506, 371, 73, 2032, 44254, 1...\n",
       "3        [45843, 3080, 11, 297, 3680, 779, 1506, 1120, ...\n",
       "4                                [710, 85, 20336, 298, 13]\n",
       "                               ...                        \n",
       "20475       [1120, 11, 408, 20445, 84, 375, 360, 9120, 13]\n",
       "20476                                     [5947, 4481, 13]\n",
       "20477                         [14828, 297, 432, 19816, 13]\n",
       "20478    [635, 331, 1479, 1506, 281, 1571, 350, 378, 34...\n",
       "20479    [4038, 1771, 350, 6, 77, 6, 64, 10236, 72, 188...\n",
       "Name: croatian, Length: 20480, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "croatian_ids=df['croatian'].str.lower().apply(lambda s: wh_tok.encode(s, add_special_tokens=False))\n",
    "croatian_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [Ġl, j, ub, av, .]\n",
       "1                                 [Ġk, ', n, ', a, ĠÄį, i]\n",
       "2                [Ġ, nd, ati, Ġje, Ġv, j, ero, jat, no, .]\n",
       "3        [Ġngo, vo, ,, Ġn, ga, vel, Ġje, Ġda, Ġz, an, a...\n",
       "4                                      [Ġz, v, lez, om, .]\n",
       "                               ...                        \n",
       "20475             [Ġda, ,, Ġne, Ġlev, u, Ġli, Ġdo, bro, .]\n",
       "20476                                       [Ġnav, rat, .]\n",
       "20477                              [Ġbor, Ġn, ge, Ġano, .]\n",
       "20478    [Ġla, ver, de, Ġje, Ġto, go, Ġk, od, ir, ava, ...\n",
       "20479            [Ġur, no, Ġk, ', n, ', a, Äį, i, Ġap, ri]\n",
       "Name: croatian, Length: 20480, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "croatian_tokenized=croatian_ids.apply(wh_tok.convert_ids_to_tokens)\n",
    "croatian_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257589,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "croatian_tk_concat=np.concatenate(croatian_tokenized)\n",
    "croatian_tk_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".        15875\n",
       ",        12673\n",
       "Ġu        8106\n",
       "v         7177\n",
       "j         6583\n",
       "         ...  \n",
       "edor         1\n",
       "ahr          1\n",
       "late         1\n",
       "bek          1\n",
       "Ġvere        1\n",
       "Name: count, Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(croatian_tk_concat).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of only the Whisper tokens that are used and map their ids to their index in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2549,), (2549,))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "croatian_i2whisper_tok=pd.Series(croatian_tk_concat).unique()\n",
    "croatian_i2whisper_tok_id=pd.Series(np.concatenate(croatian_ids)).unique()\n",
    "croatian_i2whisper_tok.shape, croatian_i2whisper_tok_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ġvere', 16443)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "croatian_i2whisper_tok[2548], croatian_i2whisper_tok_id[2548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_tok2croatian_i={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croatian_i_shrunk=croatian_ids.apply(\n",
    "    lambda l:[croatian_i2whisper_tok_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plop in code from `token_alignment.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_tensors(ipa_seqs, whisper_seqs):\n",
    "    ipa_vocab_size = np.max(np.concatenate(ipa_seqs))+1\n",
    "    whisper_vocab_size = np.max(np.concatenate(whisper_seqs))+1\n",
    "\n",
    "    eps=1e-8 # avoid division by zero\n",
    "    sum_sqd_dist = np.full([whisper_vocab_size, ipa_vocab_size], eps)\n",
    "    ipa_token_counts = np.full(ipa_vocab_size, eps)\n",
    "    return sum_sqd_dist, ipa_token_counts\n",
    "\n",
    "def initialize_probs(ipa_seqs, whisper_seqs, sum_sqd_dist, ipa_token_counts):\n",
    "    for ipa_seq, whisper_seq in zip(ipa_seqs, whisper_seqs):\n",
    "        for i, ipa_token in enumerate(ipa_seq):\n",
    "            ipa_token_counts[ipa_token]+=1\n",
    "            for j, whisper_token in enumerate(whisper_seq):\n",
    "                i_relative=i/len(ipa_seq)\n",
    "                j_relative=j/len(whisper_seq)\n",
    "                sum_sqd_dist[whisper_token,ipa_token]+=(i_relative-j_relative)**2\n",
    "    prob_unnorm = sum_sqd_dist/(ipa_token_counts)\n",
    "    sum_prob_unnorm = prob_unnorm.sum(axis=0)\n",
    "    prob_norm=prob_unnorm/sum_prob_unnorm\n",
    "    return prob_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50166, 800), (800,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_sqd_dist, ipa_token_counts = get_prob_tensors(tira_ids, croatian_ids)\n",
    "sum_sqd_dist.shape, ipa_token_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_mat=initialize_probs(tira_ids, croatian_ids, sum_sqd_dist, ipa_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.22386253e-11, 1.98550581e-05, 1.25890062e-03, ...,\n",
       "        8.70626678e-12, 4.25988304e-11, 5.65416098e-11],\n",
       "       [6.22386253e-11, 1.98550581e-05, 2.23804550e-11, ...,\n",
       "        8.70626678e-12, 4.25988304e-11, 5.65416098e-11],\n",
       "       [6.22386253e-11, 1.98550581e-05, 2.23804550e-11, ...,\n",
       "        8.70626678e-12, 4.25988304e-11, 5.65416098e-11],\n",
       "       ...,\n",
       "       [6.22386253e-11, 1.98550581e-05, 2.23804550e-11, ...,\n",
       "        8.70626678e-12, 4.25988304e-11, 5.65416098e-11],\n",
       "       [6.22386253e-11, 1.98550581e-05, 2.23804550e-11, ...,\n",
       "        8.70626678e-12, 4.25988304e-11, 5.65416098e-11],\n",
       "       [4.34136541e-06, 1.98550581e-05, 1.15033897e-03, ...,\n",
       "        5.04662104e-04, 1.60917532e-01, 2.87780518e-02]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
