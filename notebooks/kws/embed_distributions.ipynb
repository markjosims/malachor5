{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90f24387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate.hmm import SparseHMM\n",
    "from pomegranate.distributions import Categorical\n",
    "from pomegranate.distributions._distribution import Distribution\n",
    "from pomegranate._utils import _cast_as_tensor\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir(r'C:\\projects\\malachor5')\n",
    "sys.path.append(\"scripts\")\n",
    "from kws import get_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47133783",
   "metadata": {},
   "source": [
    "# KWS inference with HMMs\n",
    "Need to implement the following functionality:\n",
    "- Initialize HMM from list of keywords with one state for each word + silence + non-keyword speech\n",
    "- Set transition weights based on key phrases with small transition prob to silence/NKWS\n",
    "- Set observation probability to cosine similarity value from similarity matrix: easiest way is to either:\n",
    "    1. Set `probability` function to index $i^{th}$ item from vector of similarity values\n",
    "    2. Set `probability` function to return cosine similarity between state embedding and observed \n",
    "Let's stick with the first option, as I'll likely want to keep calculating the similarity matrix ahead of time to compute a soft-prealignment before running HMM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56541d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0014, -0.0014,    -inf])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingSimilarity(Distribution):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_embed,\n",
    "            inertia=0.0,\n",
    "            frozen=False,\n",
    "            check_data=True\n",
    "        ):\n",
    "        super().__init__(inertia=inertia, frozen=frozen, check_data=check_data)\n",
    "        self.name = \"Embedding similarity\"\n",
    "        self.state_embed = _cast_as_tensor(state_embed)\n",
    "\n",
    "    def log_probability(self, X):\n",
    "        X = _cast_as_tensor(X)\n",
    "        return torch.nn.functional.cosine_similarity(self.state_embed, X).log()\n",
    "\n",
    "state_embed = [1,   0, 1]\n",
    "close_embed = [0.9, 0, 1]\n",
    "orth_embed =  [0,   1, 0]\n",
    "embedsim = EmbeddingSimilarity(state_embed=state_embed)\n",
    "embedsim.log_probability([close_embed, close_embed, orth_embed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3a7ab55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9991, 0.8253],\n",
       "        [0.8253, 0.9991],\n",
       "        [0.0000, 0.0000]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KeySimilarityMatrix(Distribution):\n",
    "    def __init__(\n",
    "            self,\n",
    "            col_i: int,\n",
    "            max_i: int,\n",
    "            inertia=0.0,\n",
    "            frozen=False,\n",
    "            check_data=True\n",
    "        ):\n",
    "        super().__init__(inertia=inertia, frozen=frozen, check_data=check_data)\n",
    "        self.col_i = col_i\n",
    "        self.d = max_i\n",
    "        self.name = \"KeySimilarityMatrix\"\n",
    "    \n",
    "    def log_probability(self, X):\n",
    "        X = _cast_as_tensor(X)\n",
    "        return X[:,self.col_i].log()\n",
    "    \n",
    "    def _reset_cache(self):\n",
    "        return\n",
    "\n",
    "keysim1 = KeySimilarityMatrix(0, 2)\n",
    "keysim2 = KeySimilarityMatrix(1, 2)\n",
    "state1_embed = [0.5, 1,   0]\n",
    "state2_embed = [1,   0.5, 0]\n",
    "close1_embed = [0.5, 0.9,   0]\n",
    "close2_embed = [0.9, 0.5, 0]\n",
    "orth_embed =   [0,   0,   1]\n",
    "\n",
    "simmat = get_similarity_matrix(\n",
    "    torch.tensor([close1_embed, close2_embed, orth_embed]),\n",
    "    torch.tensor([state1_embed, state2_embed]),\n",
    ")\n",
    "simmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f1bd127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0009, -0.1920,    -inf]), tensor([-0.1920, -0.0009,    -inf]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysim1.log_probability(simmat), keysim2.log_probability(simmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5faba3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = SparseHMM(\n",
    "    distributions=[keysim1, keysim2],\n",
    "    edges=[\n",
    "        [keysim1,keysim1,0.5],\n",
    "        [keysim1,keysim2,0.5],\n",
    "        [keysim2,keysim2,0.5],\n",
    "        [keysim2,keysim1,0.5],\n",
    "    ],\n",
    "    starts=[0.5, 0.5],\n",
    "    ends=[0.5, 0.5],\n",
    ")\n",
    "X = get_similarity_matrix(\n",
    "    torch.tensor([close1_embed, close2_embed, close2_embed, close1_embed]),\n",
    "    torch.tensor([state1_embed, state2_embed]),\n",
    ")\n",
    "hmm.viterbi(X.unsqueeze(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
