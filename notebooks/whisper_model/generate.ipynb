{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/markjos/projects/malachor5')\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from transcribe_longform import load_and_resample\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting stereo wav to mono\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-1.2207e-03,  6.1035e-05,  2.1362e-03,  ..., -1.7853e-02,\n",
       "         -1.6937e-02, -1.6815e-02]),\n",
       " torch.Size([981440]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path='test/data/sample_biling.wav'\n",
    "wav=load_and_resample(audio_path, flatten=True)\n",
    "wav, wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')\n",
    "proc = WhisperProcessor.from_pretrained('openai/whisper-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[-0.4034,  0.0197,  0.2446,  ...,  0.1825,  0.2455,  0.0350],\n",
       "         [-0.2533,  0.0049,  0.1865,  ...,  0.3097,  0.2983,  0.1435],\n",
       "         [-0.1432, -0.1228,  0.0022,  ...,  0.3199,  0.2202,  0.0382],\n",
       "         ...,\n",
       "         [-0.4822, -0.4822, -0.4822,  ..., -0.4822, -0.4822, -0.4822],\n",
       "         [-0.4822, -0.4822, -0.4822,  ..., -0.4822, -0.4822, -0.4822],\n",
       "         [-0.4822, -0.4822, -0.4822,  ..., -0.4822, -0.4822, -0.4822]]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = proc(wav, return_tensors='pt', sampling_rate=16_000)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language='english'\tinit_tokens=tensor([[50258, 50259, 50359, 50363]])\n",
      "decoder_input_ids=tensor([[50258, 50259, 50359, 50363]])\n",
      "language='swahili'\tinit_tokens=tensor([[50258, 50318, 50359, 50363]])\n",
      "decoder_input_ids=tensor([[50258, 50318, 50359, 50363]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<|startoftranscript|><|en|><|transcribe|><|notimestamps|> If you want to describe this picture, first of all, is it possible to say the verbina? So it pulled it meaning that the sheet pulled the dog.',\n",
       " '<|startoftranscript|><|sw|><|transcribe|><|notimestamps|> nabulei')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_toks=model.generate(**inputs, language='english')\n",
    "sw_toks=model.generate(**inputs, language='swahili')\n",
    "en_str=proc.tokenizer.decode(en_toks.squeeze())\n",
    "sw_str=proc.tokenizer.decode(sw_toks.squeeze())\n",
    "en_str, sw_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language='swahili'\tinit_tokens=tensor([[50258, 50318, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50318, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50318, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50318, 50359]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language='english'\tinit_tokens=tensor([[50258, 50259, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50259, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50259, 50359]])\n",
      "decoder_input_ids=tensor([[50258, 50259, 50359]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' siya siya siya siya siya siya siya siya siya siya siya siya siya siya ia ia ia ia ia ia ia ia ia ia ia ia Si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si, si,',\n",
       " \" If you want to describe this picture, first of all, is it possible to say the verbela? So it pulled it meaning that the sheet pulled the dog. Yeah, the verbela? Oh really? It's the dog. Oh yeah, the verbela is the verbela. Yeah, that's okay. Yeah, that's okay. So just to make sure that it means it pulled it away. Yeah, she would do the pulling. And if you want to say it pulled it away, but you want to talk about the dog. Okay.\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('automatic-speech-recognition', model='openai/whisper-tiny')\n",
    "sw_pipe_out=pipe(audio_path, return_timestamps=True, generate_kwargs={'language':'swahili'})\n",
    "en_pipe_out=pipe(audio_path, return_timestamps=True, generate_kwargs={'language':'english'})\n",
    "sw_pipe_out['text'], en_pipe_out['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
