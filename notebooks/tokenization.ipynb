{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo special tokens in Spanish and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "sp_tok = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\")\n",
    "en_tok = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50262, 50359, 50363, 50257]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_str_sp = sp_tok.encode('')\n",
    "empty_str_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|es|><|transcribe|><|notimestamps|><|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tok.decode(empty_str_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50259, 50359, 50363, 50257]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_str_en = en_tok.encode('')\n",
    "empty_str_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|en|><|transcribe|><|notimestamps|><|endoftext|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.decode(empty_str_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do Spanish and English tokenizers differ at all when decoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|startoftranscript|><|en|><|transcribe|><|notimestamps|><|endoftext|>',\n",
       " '<|startoftranscript|><|en|><|transcribe|><|notimestamps|><|endoftext|>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.decode(empty_str_en), sp_tok.decode(empty_str_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|startoftranscript|><|es|><|transcribe|><|notimestamps|><|endoftext|>',\n",
       " '<|startoftranscript|><|es|><|transcribe|><|notimestamps|><|endoftext|>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.decode(empty_str_sp), sp_tok.decode(empty_str_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.special_tokens_map == sp_tok.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.all_special_ids == sp_tok.all_special_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo decoding non-special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_ids = lambda l: [tok_id for tok_id in l if tok_id not in en_tok.all_special_ids]\n",
    "remove_special_ids(empty_str_en), remove_special_ids(empty_str_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15947], [15947])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_ids(\n",
    "    en_tok.encode(\"Hello\")\n",
    "), remove_special_ids(\n",
    "    sp_tok.encode(\"Hello\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48529], [48529])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_ids(\n",
    "    en_tok.encode(\"Hola\")\n",
    "), remove_special_ids(\n",
    "    sp_tok.encode(\"Hola\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the only difference between tokenizers for different languages is what lang id token is output when encoding, so I won't worry about comparing them any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15947, 7751]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_no_special = lambda s: remove_special_ids(en_tok.encode(s))\n",
    "encode_no_special(\"Hello hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 664, 82, 7197]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_no_special(\"Dogs dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello, world\n",
      "15947\tHello\tb'Hello'\n",
      "11\t,\tb','\n",
      "1002\t world\tb' world'\n",
      "Input: Hello world\n",
      "15947\tHello\tb'Hello'\n",
      "1002\t world\tb' world'\n",
      "Input: Helloworld\n",
      "39\tH\tb'H'\n",
      "21348\tellow\tb'ellow'\n",
      "3445\torld\tb'orld'\n"
     ]
    }
   ],
   "source": [
    "def print_tokens(s):\n",
    "    print(\"Input:\", s)\n",
    "    tok_ids = encode_no_special(s)\n",
    "    for tok_id in tok_ids:\n",
    "        tok = en_tok.decode(tok_id)\n",
    "        print(f\"{tok_id}\\t{tok}\\t{tok.encode('raw_unicode_escape')}\")\n",
    "\n",
    "print_tokens(\"Hello, world\")\n",
    "print_tokens(\"Hello world\")\n",
    "print_tokens(\"Helloworld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Bom dia meus caras\n",
      "33\tB\tb'B'\n",
      "298\tom\tb'om'\n",
      "6801\t dia\tb' dia'\n",
      "28033\t meus\tb' meus'\n",
      "1032\t car\tb' car'\n",
      "296\tas\tb'as'\n"
     ]
    }
   ],
   "source": [
    "print_tokens(\"Bom dia meus caras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: да свидания\n",
      "3444\tда\tb'\\\\u0434\\\\u0430'\n",
      "43666\t свид\tb' \\\\u0441\\\\u0432\\\\u0438\\\\u0434'\n",
      "8831\tания\tb'\\\\u0430\\\\u043d\\\\u0438\\\\u044f'\n"
     ]
    }
   ],
   "source": [
    "print_tokens(\"да свидания\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: àpɾí jícə̀lò\n",
      "64\ta\tb'a'\n",
      "136\t�\tb'\\\\ufffd'\n",
      "222\t�\tb'\\\\ufffd'\n",
      "79\tp\tb'p'\n",
      "133\t�\tb'\\\\ufffd'\n",
      "122\t�\tb'\\\\ufffd'\n",
      "72\ti\tb'i'\n",
      "32797\t́\tb'\\\\u0301'\n",
      "32606\t ji\tb' ji'\n",
      "32797\t́\tb'\\\\u0301'\n",
      "66\tc\tb'c'\n",
      "7250\tə\tb'\\\\u0259'\n",
      "136\t�\tb'\\\\ufffd'\n",
      "222\t�\tb'\\\\ufffd'\n",
      "752\tlo\tb'lo'\n",
      "136\t�\tb'\\\\ufffd'\n",
      "222\t�\tb'\\\\ufffd'\n"
     ]
    }
   ],
   "source": [
    "print_tokens(\"àpɾí jícə̀lò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'àpɾí jícə̀lò'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec = lambda s: en_tok.decode(en_tok.encode(s), skip_special_tokens=True)\n",
    "enc_dec(\"àpɾí jícə̀lò\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change name of language in Whisper tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic_tok = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", task=\"transcribe\", language=\"english\")\n",
    "tic_tok.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50258, 50259, 50359, 50363, 220, 50257]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tic_tok.\n",
    "tic_tok.encode(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
